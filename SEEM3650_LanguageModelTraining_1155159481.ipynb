{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fad486466934ee5b474d2f63f380b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10585e296bf4416e8ed38a44f9b1dda4",
              "IPY_MODEL_60cdffbe29134cb197bb8c419e1e8fea",
              "IPY_MODEL_f9fe8d7c591046c68576ba3318f0621a"
            ],
            "layout": "IPY_MODEL_87d49bb21b2e4889a49db8cdcf6a3325"
          }
        },
        "10585e296bf4416e8ed38a44f9b1dda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d898204d6a473fa953df8a868f85c5",
            "placeholder": "​",
            "style": "IPY_MODEL_5e97ffce70344b45a7737ccb67c43f52",
            "value": "README.md: 100%"
          }
        },
        "60cdffbe29134cb197bb8c419e1e8fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c266a94807349588aa7745bf4c4358f",
            "max": 7537,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9794d1f79a4440a496c6351cec632e0c",
            "value": 7537
          }
        },
        "f9fe8d7c591046c68576ba3318f0621a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c1c611575bc40c493a83136e7a9a896",
            "placeholder": "​",
            "style": "IPY_MODEL_223bdceb505443829136c47ab9b4a563",
            "value": " 7.54k/7.54k [00:00&lt;00:00, 777kB/s]"
          }
        },
        "87d49bb21b2e4889a49db8cdcf6a3325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d898204d6a473fa953df8a868f85c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e97ffce70344b45a7737ccb67c43f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c266a94807349588aa7745bf4c4358f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9794d1f79a4440a496c6351cec632e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c1c611575bc40c493a83136e7a9a896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223bdceb505443829136c47ab9b4a563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "191d22a8569244d7a35720cf84bd43fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_680cf774d2594723ae8d7398565fef4a",
              "IPY_MODEL_454bb85e5c4a4cf7a5ee86323f4f0e75",
              "IPY_MODEL_8e28c8eda7944d219257a892cfecda72"
            ],
            "layout": "IPY_MODEL_16f051b0dd364cf3b35127a39c12134e"
          }
        },
        "680cf774d2594723ae8d7398565fef4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77e300774fc4853949429cd19818040",
            "placeholder": "​",
            "style": "IPY_MODEL_8b56b66ee6254b72bcaf1a94450ec4ce",
            "value": "github-code.py: 100%"
          }
        },
        "454bb85e5c4a4cf7a5ee86323f4f0e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba110ee29f449039fdb0e1ee6685dd1",
            "max": 7226,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f470ce7540f740b8b7779f98a028faa3",
            "value": 7226
          }
        },
        "8e28c8eda7944d219257a892cfecda72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4997c16b34a4182aaf4f01b1563dea0",
            "placeholder": "​",
            "style": "IPY_MODEL_8416061564c64787875b0ecf76803e2c",
            "value": " 7.23k/7.23k [00:00&lt;00:00, 703kB/s]"
          }
        },
        "16f051b0dd364cf3b35127a39c12134e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77e300774fc4853949429cd19818040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b56b66ee6254b72bcaf1a94450ec4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba110ee29f449039fdb0e1ee6685dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f470ce7540f740b8b7779f98a028faa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4997c16b34a4182aaf4f01b1563dea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8416061564c64787875b0ecf76803e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXO8pt-1xkWg",
        "outputId": "a5d537a2-fdc8-4b4d-fd33-19943321665f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 3.97 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5uaJ8DOynIE",
        "outputId": "f7e60a16-7678-4bc3-8d76-ffbd524c29bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZR0jWq3yqNr",
        "outputId": "d2566be2-c640-4c31-99b2-a05163ff290d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-OWjCKRyw7n",
        "outputId": "ae231a67-c6f7-449f-c8ef-389634858558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Connect to GPU and Check:"
      ],
      "metadata": {
        "id": "C2iq94RpVGvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cYLmtOn1_Nq",
        "outputId": "e475ab35-e485-4453-a32c-b5ef874bd67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --dtype=float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZs0CdHly-nd",
        "outputId": "fcebed6b-3f4b-47dc-f1f0-301ba6a7b67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0426 04:14:26.559000 2739 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2664, time 49952.47ms, mfu -100.00%\n",
            "iter 10: loss 3.1456, time 81.68ms, mfu 4.56%\n",
            "iter 20: loss 2.7351, time 80.39ms, mfu 4.57%\n",
            "iter 30: loss 2.6207, time 82.13ms, mfu 4.57%\n",
            "iter 40: loss 2.5725, time 83.86ms, mfu 4.55%\n",
            "iter 50: loss 2.5225, time 82.19ms, mfu 4.55%\n",
            "iter 60: loss 2.5078, time 83.06ms, mfu 4.55%\n",
            "iter 70: loss 2.4898, time 82.46ms, mfu 4.54%\n",
            "iter 80: loss 2.4992, time 85.61ms, mfu 4.52%\n",
            "iter 90: loss 2.4622, time 82.45ms, mfu 4.52%\n",
            "iter 100: loss 2.4617, time 81.89ms, mfu 4.53%\n",
            "iter 110: loss 2.4475, time 85.25ms, mfu 4.51%\n",
            "iter 120: loss 2.4286, time 81.76ms, mfu 4.52%\n",
            "iter 130: loss 2.4194, time 84.61ms, mfu 4.50%\n",
            "iter 140: loss 2.3964, time 82.23ms, mfu 4.51%\n",
            "iter 150: loss 2.4133, time 83.72ms, mfu 4.50%\n",
            "iter 160: loss 2.3694, time 84.78ms, mfu 4.49%\n",
            "iter 170: loss 2.3517, time 84.82ms, mfu 4.48%\n",
            "iter 180: loss 2.3093, time 84.50ms, mfu 4.47%\n",
            "iter 190: loss 2.2574, time 84.58ms, mfu 4.47%\n",
            "iter 200: loss 2.2131, time 84.37ms, mfu 4.46%\n",
            "iter 210: loss 2.1492, time 83.49ms, mfu 4.46%\n",
            "iter 220: loss 2.1434, time 85.44ms, mfu 4.45%\n",
            "iter 230: loss 2.0784, time 84.80ms, mfu 4.45%\n",
            "iter 240: loss 2.0760, time 83.87ms, mfu 4.45%\n",
            "step 250: train loss 1.9682, val loss 2.0652\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0407, time 10734.74ms, mfu 4.00%\n",
            "iter 260: loss 1.9726, time 84.26ms, mfu 4.05%\n",
            "iter 270: loss 1.9734, time 84.19ms, mfu 4.08%\n",
            "iter 280: loss 1.9769, time 84.54ms, mfu 4.12%\n",
            "iter 290: loss 1.9213, time 86.60ms, mfu 4.14%\n",
            "iter 300: loss 1.9028, time 90.95ms, mfu 4.13%\n",
            "iter 310: loss 1.8640, time 89.78ms, mfu 4.13%\n",
            "iter 320: loss 1.8558, time 89.62ms, mfu 4.14%\n",
            "iter 330: loss 1.8141, time 86.44ms, mfu 4.15%\n",
            "iter 340: loss 1.7797, time 87.75ms, mfu 4.16%\n",
            "iter 350: loss 1.8333, time 89.09ms, mfu 4.16%\n",
            "iter 360: loss 1.7746, time 87.26ms, mfu 4.18%\n",
            "iter 370: loss 1.7400, time 87.05ms, mfu 4.19%\n",
            "iter 380: loss 1.7271, time 89.20ms, mfu 4.18%\n",
            "iter 390: loss 1.7362, time 89.47ms, mfu 4.18%\n",
            "iter 400: loss 1.7638, time 89.95ms, mfu 4.18%\n",
            "iter 410: loss 1.6865, time 90.20ms, mfu 4.17%\n",
            "iter 420: loss 1.7146, time 88.45ms, mfu 4.18%\n",
            "iter 430: loss 1.6831, time 90.60ms, mfu 4.17%\n",
            "iter 440: loss 1.6600, time 88.16ms, mfu 4.18%\n",
            "iter 450: loss 1.6472, time 88.47ms, mfu 4.18%\n",
            "iter 460: loss 1.5875, time 89.70ms, mfu 4.18%\n",
            "iter 470: loss 1.6481, time 90.24ms, mfu 4.17%\n",
            "iter 480: loss 1.6183, time 88.76ms, mfu 4.18%\n",
            "iter 490: loss 1.5948, time 88.66ms, mfu 4.18%\n",
            "step 500: train loss 1.5218, val loss 1.7253\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5919, time 11246.39ms, mfu 3.76%\n",
            "iter 510: loss 1.6093, time 88.68ms, mfu 3.81%\n",
            "iter 520: loss 1.5833, time 90.51ms, mfu 3.84%\n",
            "iter 530: loss 1.5534, time 88.37ms, mfu 3.88%\n",
            "iter 540: loss 1.6078, time 91.48ms, mfu 3.90%\n",
            "iter 550: loss 1.5594, time 90.07ms, mfu 3.92%\n",
            "iter 560: loss 1.5719, time 93.70ms, mfu 3.93%\n",
            "iter 570: loss 1.5681, time 93.07ms, mfu 3.93%\n",
            "iter 580: loss 1.5331, time 95.20ms, mfu 3.93%\n",
            "iter 590: loss 1.4931, time 95.30ms, mfu 3.93%\n",
            "iter 600: loss 1.5060, time 93.14ms, mfu 3.94%\n",
            "iter 610: loss 1.5392, time 94.53ms, mfu 3.94%\n",
            "iter 620: loss 1.5274, time 94.32ms, mfu 3.94%\n",
            "iter 630: loss 1.5030, time 95.33ms, mfu 3.94%\n",
            "iter 640: loss 1.4555, time 93.64ms, mfu 3.94%\n",
            "iter 650: loss 1.4944, time 93.25ms, mfu 3.95%\n",
            "iter 660: loss 1.5008, time 92.40ms, mfu 3.95%\n",
            "iter 670: loss 1.4368, time 93.77ms, mfu 3.96%\n",
            "iter 680: loss 1.5013, time 94.90ms, mfu 3.95%\n",
            "iter 690: loss 1.4555, time 94.51ms, mfu 3.95%\n",
            "iter 700: loss 1.4747, time 94.54ms, mfu 3.95%\n",
            "iter 710: loss 1.4463, time 96.54ms, mfu 3.94%\n",
            "iter 720: loss 1.4436, time 96.83ms, mfu 3.93%\n",
            "iter 730: loss 1.4151, time 96.73ms, mfu 3.92%\n",
            "iter 740: loss 1.4232, time 98.84ms, mfu 3.91%\n",
            "step 750: train loss 1.3551, val loss 1.5840\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4124, time 11620.54ms, mfu 3.52%\n",
            "iter 760: loss 1.4347, time 91.14ms, mfu 3.58%\n",
            "iter 770: loss 1.4212, time 90.23ms, mfu 3.63%\n",
            "iter 780: loss 1.4167, time 89.32ms, mfu 3.69%\n",
            "iter 790: loss 1.4173, time 93.17ms, mfu 3.72%\n",
            "iter 800: loss 1.4266, time 90.49ms, mfu 3.76%\n",
            "iter 810: loss 1.3977, time 91.83ms, mfu 3.79%\n",
            "iter 820: loss 1.3998, time 94.46ms, mfu 3.80%\n",
            "iter 830: loss 1.3849, time 91.64ms, mfu 3.83%\n",
            "iter 840: loss 1.3944, time 91.81ms, mfu 3.85%\n",
            "iter 850: loss 1.3814, time 94.70ms, mfu 3.86%\n",
            "iter 860: loss 1.3873, time 92.17ms, mfu 3.88%\n",
            "iter 870: loss 1.3903, time 94.33ms, mfu 3.89%\n",
            "iter 880: loss 1.3684, time 94.04ms, mfu 3.89%\n",
            "iter 890: loss 1.3805, time 92.33ms, mfu 3.91%\n",
            "iter 900: loss 1.3702, time 91.46ms, mfu 3.92%\n",
            "iter 910: loss 1.3138, time 90.78ms, mfu 3.94%\n",
            "iter 920: loss 1.3620, time 91.91ms, mfu 3.95%\n",
            "iter 930: loss 1.3559, time 90.62ms, mfu 3.97%\n",
            "iter 940: loss 1.3385, time 91.28ms, mfu 3.98%\n",
            "iter 950: loss 1.3434, time 92.10ms, mfu 3.99%\n",
            "iter 960: loss 1.3502, time 91.82ms, mfu 3.99%\n",
            "iter 970: loss 1.3544, time 95.03ms, mfu 3.99%\n",
            "iter 980: loss 1.3514, time 93.33ms, mfu 3.99%\n",
            "iter 990: loss 1.3289, time 92.05ms, mfu 3.99%\n",
            "step 1000: train loss 1.2659, val loss 1.5167\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3278, time 11362.79ms, mfu 3.60%\n",
            "iter 1010: loss 1.3342, time 88.48ms, mfu 3.66%\n",
            "iter 1020: loss 1.3042, time 88.48ms, mfu 3.71%\n",
            "iter 1030: loss 1.3273, time 89.12ms, mfu 3.76%\n",
            "iter 1040: loss 1.3470, time 89.69ms, mfu 3.80%\n",
            "iter 1050: loss 1.2834, time 91.06ms, mfu 3.83%\n",
            "iter 1060: loss 1.3347, time 90.12ms, mfu 3.86%\n",
            "iter 1070: loss 1.3325, time 90.61ms, mfu 3.89%\n",
            "iter 1080: loss 1.3334, time 94.82ms, mfu 3.89%\n",
            "iter 1090: loss 1.3435, time 93.60ms, mfu 3.90%\n",
            "iter 1100: loss 1.3133, time 93.92ms, mfu 3.91%\n",
            "iter 1110: loss 1.2941, time 91.72ms, mfu 3.92%\n",
            "iter 1120: loss 1.2872, time 90.85ms, mfu 3.94%\n",
            "iter 1130: loss 1.2929, time 91.34ms, mfu 3.95%\n",
            "iter 1140: loss 1.2935, time 89.73ms, mfu 3.97%\n",
            "iter 1150: loss 1.3066, time 92.33ms, mfu 3.98%\n",
            "iter 1160: loss 1.3271, time 93.24ms, mfu 3.98%\n",
            "iter 1170: loss 1.2922, time 91.27ms, mfu 3.99%\n",
            "iter 1180: loss 1.3127, time 92.93ms, mfu 3.99%\n",
            "iter 1190: loss 1.2602, time 92.73ms, mfu 4.00%\n",
            "iter 1200: loss 1.2860, time 91.54ms, mfu 4.00%\n",
            "iter 1210: loss 1.2645, time 92.44ms, mfu 4.01%\n",
            "iter 1220: loss 1.3026, time 91.89ms, mfu 4.01%\n",
            "iter 1230: loss 1.2916, time 94.10ms, mfu 4.01%\n",
            "iter 1240: loss 1.2918, time 92.34ms, mfu 4.01%\n",
            "step 1250: train loss 1.1989, val loss 1.4842\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2763, time 11425.70ms, mfu 3.61%\n",
            "iter 1260: loss 1.2776, time 89.22ms, mfu 3.67%\n",
            "iter 1270: loss 1.2659, time 90.30ms, mfu 3.71%\n",
            "iter 1280: loss 1.2514, time 90.15ms, mfu 3.76%\n",
            "iter 1290: loss 1.2718, time 91.40ms, mfu 3.79%\n",
            "iter 1300: loss 1.2968, time 90.91ms, mfu 3.82%\n",
            "iter 1310: loss 1.2294, time 89.93ms, mfu 3.85%\n",
            "iter 1320: loss 1.2948, time 93.04ms, mfu 3.87%\n",
            "iter 1330: loss 1.2604, time 94.96ms, mfu 3.87%\n",
            "iter 1340: loss 1.2955, time 93.75ms, mfu 3.88%\n",
            "iter 1350: loss 1.2479, time 94.24ms, mfu 3.89%\n",
            "iter 1360: loss 1.2623, time 93.50ms, mfu 3.90%\n",
            "iter 1370: loss 1.2591, time 93.07ms, mfu 3.91%\n",
            "iter 1380: loss 1.2610, time 94.55ms, mfu 3.91%\n",
            "iter 1390: loss 1.2437, time 92.44ms, mfu 3.92%\n",
            "iter 1400: loss 1.2525, time 92.69ms, mfu 3.93%\n",
            "iter 1410: loss 1.2461, time 90.97ms, mfu 3.95%\n",
            "iter 1420: loss 1.2659, time 91.67ms, mfu 3.96%\n",
            "iter 1430: loss 1.2373, time 94.19ms, mfu 3.96%\n",
            "iter 1440: loss 1.2515, time 94.11ms, mfu 3.96%\n",
            "iter 1450: loss 1.2234, time 94.44ms, mfu 3.96%\n",
            "iter 1460: loss 1.2416, time 92.27ms, mfu 3.97%\n",
            "iter 1470: loss 1.2231, time 95.53ms, mfu 3.96%\n",
            "iter 1480: loss 1.2097, time 92.23ms, mfu 3.97%\n",
            "iter 1490: loss 1.2314, time 94.54ms, mfu 3.97%\n",
            "step 1500: train loss 1.1475, val loss 1.4677\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1830, time 11497.46ms, mfu 3.57%\n",
            "iter 1510: loss 1.2307, time 90.26ms, mfu 3.63%\n",
            "iter 1520: loss 1.2208, time 90.31ms, mfu 3.68%\n",
            "iter 1530: loss 1.2503, time 90.38ms, mfu 3.72%\n",
            "iter 1540: loss 1.1843, time 91.64ms, mfu 3.76%\n",
            "iter 1550: loss 1.2326, time 89.68ms, mfu 3.80%\n",
            "iter 1560: loss 1.2088, time 90.87ms, mfu 3.83%\n",
            "iter 1570: loss 1.2338, time 92.70ms, mfu 3.85%\n",
            "iter 1580: loss 1.2043, time 93.10ms, mfu 3.86%\n",
            "iter 1590: loss 1.1842, time 93.88ms, mfu 3.87%\n",
            "iter 1600: loss 1.1996, time 95.07ms, mfu 3.88%\n",
            "iter 1610: loss 1.2305, time 93.73ms, mfu 3.89%\n",
            "iter 1620: loss 1.1798, time 92.32ms, mfu 3.90%\n",
            "iter 1630: loss 1.1960, time 94.10ms, mfu 3.91%\n",
            "iter 1640: loss 1.1982, time 92.47ms, mfu 3.92%\n",
            "iter 1650: loss 1.1806, time 92.09ms, mfu 3.93%\n",
            "iter 1660: loss 1.2162, time 93.06ms, mfu 3.94%\n",
            "iter 1670: loss 1.1901, time 90.85ms, mfu 3.96%\n",
            "iter 1680: loss 1.1973, time 92.91ms, mfu 3.96%\n",
            "iter 1690: loss 1.1934, time 92.37ms, mfu 3.97%\n",
            "iter 1700: loss 1.1726, time 92.93ms, mfu 3.97%\n",
            "iter 1710: loss 1.1822, time 93.86ms, mfu 3.97%\n",
            "iter 1720: loss 1.1734, time 94.01ms, mfu 3.97%\n",
            "iter 1730: loss 1.1958, time 91.48ms, mfu 3.98%\n",
            "iter 1740: loss 1.1612, time 91.61ms, mfu 3.99%\n",
            "step 1750: train loss 1.0968, val loss 1.4645\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1815, time 11544.47ms, mfu 3.59%\n",
            "iter 1760: loss 1.1813, time 89.29ms, mfu 3.65%\n",
            "iter 1770: loss 1.1933, time 88.25ms, mfu 3.71%\n",
            "iter 1780: loss 1.1885, time 90.74ms, mfu 3.75%\n",
            "iter 1790: loss 1.1857, time 90.88ms, mfu 3.78%\n",
            "iter 1800: loss 1.1786, time 91.89ms, mfu 3.81%\n",
            "iter 1810: loss 1.1499, time 90.57ms, mfu 3.84%\n",
            "iter 1820: loss 1.1656, time 90.13ms, mfu 3.87%\n",
            "iter 1830: loss 1.1637, time 92.67ms, mfu 3.89%\n",
            "iter 1840: loss 1.1615, time 93.97ms, mfu 3.89%\n",
            "iter 1850: loss 1.1600, time 95.71ms, mfu 3.89%\n",
            "iter 1860: loss 1.1766, time 93.24ms, mfu 3.90%\n",
            "iter 1870: loss 1.1355, time 92.14ms, mfu 3.92%\n",
            "iter 1880: loss 1.1830, time 93.86ms, mfu 3.92%\n",
            "iter 1890: loss 1.1727, time 90.53ms, mfu 3.94%\n",
            "iter 1900: loss 1.1262, time 90.69ms, mfu 3.96%\n",
            "iter 1910: loss 1.1688, time 91.16ms, mfu 3.97%\n",
            "iter 1920: loss 1.1635, time 94.32ms, mfu 3.97%\n",
            "iter 1930: loss 1.1426, time 94.19ms, mfu 3.97%\n",
            "iter 1940: loss 1.1241, time 93.35ms, mfu 3.97%\n",
            "iter 1950: loss 1.1341, time 93.52ms, mfu 3.97%\n",
            "iter 1960: loss 1.1425, time 92.47ms, mfu 3.98%\n",
            "iter 1970: loss 1.1468, time 94.66ms, mfu 3.97%\n",
            "iter 1980: loss 1.1509, time 92.89ms, mfu 3.98%\n",
            "iter 1990: loss 1.1470, time 91.70ms, mfu 3.99%\n",
            "step 2000: train loss 1.0516, val loss 1.4685\n",
            "iter 2000: loss 1.1234, time 11152.31ms, mfu 3.59%\n",
            "iter 2010: loss 1.1229, time 93.01ms, mfu 3.63%\n",
            "iter 2020: loss 1.1194, time 96.19ms, mfu 3.66%\n",
            "iter 2030: loss 1.1478, time 96.51ms, mfu 3.68%\n",
            "iter 2040: loss 1.1346, time 93.44ms, mfu 3.71%\n",
            "iter 2050: loss 1.1065, time 94.03ms, mfu 3.73%\n",
            "iter 2060: loss 1.0997, time 92.55ms, mfu 3.76%\n",
            "iter 2070: loss 1.1263, time 91.98ms, mfu 3.79%\n",
            "iter 2080: loss 1.1155, time 91.48ms, mfu 3.82%\n",
            "iter 2090: loss 1.1255, time 93.68ms, mfu 3.84%\n",
            "iter 2100: loss 1.1340, time 93.88ms, mfu 3.85%\n",
            "iter 2110: loss 1.1278, time 92.71ms, mfu 3.87%\n",
            "iter 2120: loss 1.1275, time 91.52ms, mfu 3.89%\n",
            "iter 2130: loss 1.1319, time 91.77ms, mfu 3.90%\n",
            "iter 2140: loss 1.1279, time 92.61ms, mfu 3.92%\n",
            "iter 2150: loss 1.1208, time 91.31ms, mfu 3.93%\n",
            "iter 2160: loss 1.1310, time 93.02ms, mfu 3.94%\n",
            "iter 2170: loss 1.1239, time 94.00ms, mfu 3.94%\n",
            "iter 2180: loss 1.1081, time 92.70ms, mfu 3.95%\n",
            "iter 2190: loss 1.1025, time 95.15ms, mfu 3.95%\n",
            "iter 2200: loss 1.1207, time 93.14ms, mfu 3.95%\n",
            "iter 2210: loss 1.1060, time 93.06ms, mfu 3.96%\n",
            "iter 2220: loss 1.1214, time 91.63ms, mfu 3.97%\n",
            "iter 2230: loss 1.1125, time 92.99ms, mfu 3.97%\n",
            "iter 2240: loss 1.1205, time 91.29ms, mfu 3.98%\n",
            "step 2250: train loss 1.0044, val loss 1.4836\n",
            "iter 2250: loss 1.1114, time 11156.10ms, mfu 3.59%\n",
            "iter 2260: loss 1.1035, time 90.88ms, mfu 3.64%\n",
            "iter 2270: loss 1.1205, time 93.29ms, mfu 3.67%\n",
            "iter 2280: loss 1.0881, time 94.05ms, mfu 3.70%\n",
            "iter 2290: loss 1.1390, time 94.25ms, mfu 3.73%\n",
            "iter 2300: loss 1.1205, time 91.43ms, mfu 3.76%\n",
            "iter 2310: loss 1.0858, time 91.70ms, mfu 3.79%\n",
            "iter 2320: loss 1.0945, time 93.71ms, mfu 3.81%\n",
            "iter 2330: loss 1.1021, time 94.43ms, mfu 3.82%\n",
            "iter 2340: loss 1.1101, time 93.46ms, mfu 3.84%\n",
            "iter 2350: loss 1.1115, time 96.17ms, mfu 3.84%\n",
            "iter 2360: loss 1.1010, time 92.24ms, mfu 3.86%\n",
            "iter 2370: loss 1.0832, time 93.37ms, mfu 3.88%\n",
            "iter 2380: loss 1.0852, time 92.06ms, mfu 3.89%\n",
            "iter 2390: loss 1.0724, time 93.62ms, mfu 3.90%\n",
            "iter 2400: loss 1.0789, time 95.96ms, mfu 3.90%\n",
            "iter 2410: loss 1.0661, time 93.44ms, mfu 3.91%\n",
            "iter 2420: loss 1.0696, time 92.93ms, mfu 3.92%\n",
            "iter 2430: loss 1.0425, time 92.87ms, mfu 3.93%\n",
            "iter 2440: loss 1.0585, time 92.13ms, mfu 3.94%\n",
            "iter 2450: loss 1.0643, time 91.99ms, mfu 3.95%\n",
            "iter 2460: loss 1.0838, time 93.25ms, mfu 3.96%\n",
            "iter 2470: loss 1.0796, time 93.92ms, mfu 3.96%\n",
            "iter 2480: loss 1.0847, time 91.50ms, mfu 3.97%\n",
            "iter 2490: loss 1.0487, time 92.75ms, mfu 3.97%\n",
            "step 2500: train loss 0.9548, val loss 1.4843\n",
            "iter 2500: loss 1.0809, time 11169.68ms, mfu 3.58%\n",
            "iter 2510: loss 1.0668, time 93.10ms, mfu 3.62%\n",
            "iter 2520: loss 1.0427, time 93.21ms, mfu 3.66%\n",
            "iter 2530: loss 1.0476, time 91.60ms, mfu 3.70%\n",
            "iter 2540: loss 1.0483, time 92.91ms, mfu 3.73%\n",
            "iter 2550: loss 1.0666, time 94.36ms, mfu 3.75%\n",
            "iter 2560: loss 1.0468, time 94.65ms, mfu 3.77%\n",
            "iter 2570: loss 1.0660, time 91.55ms, mfu 3.80%\n",
            "iter 2580: loss 1.0601, time 91.64ms, mfu 3.83%\n",
            "iter 2590: loss 1.0648, time 94.27ms, mfu 3.84%\n",
            "iter 2600: loss 1.0534, time 96.18ms, mfu 3.84%\n",
            "iter 2610: loss 1.0397, time 93.41ms, mfu 3.86%\n",
            "iter 2620: loss 1.0362, time 94.53ms, mfu 3.87%\n",
            "iter 2630: loss 1.0219, time 94.05ms, mfu 3.88%\n",
            "iter 2640: loss 1.0344, time 92.32ms, mfu 3.89%\n",
            "iter 2650: loss 1.0671, time 93.25ms, mfu 3.90%\n",
            "iter 2660: loss 1.0338, time 93.75ms, mfu 3.91%\n",
            "iter 2670: loss 1.0077, time 92.34ms, mfu 3.92%\n",
            "iter 2680: loss 1.0433, time 92.64ms, mfu 3.93%\n",
            "iter 2690: loss 1.0470, time 93.49ms, mfu 3.94%\n",
            "iter 2700: loss 1.0147, time 93.20ms, mfu 3.94%\n",
            "iter 2710: loss 1.0416, time 91.83ms, mfu 3.96%\n",
            "iter 2720: loss 1.0369, time 92.46ms, mfu 3.96%\n",
            "iter 2730: loss 1.0548, time 92.92ms, mfu 3.97%\n",
            "iter 2740: loss 1.0277, time 94.05ms, mfu 3.97%\n",
            "step 2750: train loss 0.9082, val loss 1.5166\n",
            "iter 2750: loss 1.0320, time 11171.11ms, mfu 3.57%\n",
            "iter 2760: loss 1.0226, time 95.93ms, mfu 3.60%\n",
            "iter 2770: loss 1.0170, time 93.41ms, mfu 3.64%\n",
            "iter 2780: loss 1.0156, time 92.17ms, mfu 3.68%\n",
            "iter 2790: loss 1.0311, time 94.68ms, mfu 3.71%\n",
            "iter 2800: loss 1.0084, time 91.72ms, mfu 3.74%\n",
            "iter 2810: loss 1.0418, time 91.38ms, mfu 3.78%\n",
            "iter 2820: loss 1.0184, time 90.66ms, mfu 3.81%\n",
            "iter 2830: loss 1.0239, time 91.99ms, mfu 3.83%\n",
            "iter 2840: loss 0.9798, time 93.25ms, mfu 3.85%\n",
            "iter 2850: loss 1.0156, time 93.65ms, mfu 3.86%\n",
            "iter 2860: loss 1.0134, time 93.91ms, mfu 3.87%\n",
            "iter 2870: loss 1.0031, time 92.08ms, mfu 3.89%\n",
            "iter 2880: loss 1.0305, time 92.35ms, mfu 3.91%\n",
            "iter 2890: loss 1.0016, time 92.73ms, mfu 3.92%\n",
            "iter 2900: loss 0.9849, time 92.59ms, mfu 3.93%\n",
            "iter 2910: loss 1.0294, time 91.16ms, mfu 3.94%\n",
            "iter 2920: loss 1.0031, time 92.17ms, mfu 3.95%\n",
            "iter 2930: loss 0.9977, time 93.90ms, mfu 3.95%\n",
            "iter 2940: loss 0.9846, time 92.35ms, mfu 3.96%\n",
            "iter 2950: loss 1.0115, time 92.77ms, mfu 3.97%\n",
            "iter 2960: loss 0.9904, time 92.55ms, mfu 3.97%\n",
            "iter 2970: loss 0.9920, time 91.93ms, mfu 3.98%\n",
            "iter 2980: loss 0.9849, time 92.07ms, mfu 3.99%\n",
            "iter 2990: loss 0.9759, time 92.87ms, mfu 3.99%\n",
            "step 3000: train loss 0.8602, val loss 1.5317\n",
            "iter 3000: loss 0.9787, time 11097.49ms, mfu 3.60%\n",
            "iter 3010: loss 0.9954, time 96.11ms, mfu 3.62%\n",
            "iter 3020: loss 0.9932, time 96.90ms, mfu 3.65%\n",
            "iter 3030: loss 0.9994, time 92.51ms, mfu 3.68%\n",
            "iter 3040: loss 1.0231, time 92.09ms, mfu 3.72%\n",
            "iter 3050: loss 0.9790, time 91.90ms, mfu 3.75%\n",
            "iter 3060: loss 0.9911, time 93.78ms, mfu 3.78%\n",
            "iter 3070: loss 1.0151, time 91.72ms, mfu 3.80%\n",
            "iter 3080: loss 0.9840, time 94.04ms, mfu 3.82%\n",
            "iter 3090: loss 0.9815, time 93.96ms, mfu 3.83%\n",
            "iter 3100: loss 1.0001, time 92.48ms, mfu 3.85%\n",
            "iter 3110: loss 0.9656, time 94.34ms, mfu 3.86%\n",
            "iter 3120: loss 0.9946, time 90.43ms, mfu 3.89%\n",
            "iter 3130: loss 0.9729, time 91.46ms, mfu 3.91%\n",
            "iter 3140: loss 0.9673, time 93.71ms, mfu 3.91%\n",
            "iter 3150: loss 0.9953, time 93.21ms, mfu 3.92%\n",
            "iter 3160: loss 0.9942, time 93.47ms, mfu 3.93%\n",
            "iter 3170: loss 0.9611, time 93.18ms, mfu 3.94%\n",
            "iter 3180: loss 0.9740, time 92.97ms, mfu 3.94%\n",
            "iter 3190: loss 0.9865, time 91.34ms, mfu 3.96%\n",
            "iter 3200: loss 0.9620, time 93.10ms, mfu 3.96%\n",
            "iter 3210: loss 0.9528, time 94.11ms, mfu 3.96%\n",
            "iter 3220: loss 0.9551, time 93.69ms, mfu 3.96%\n",
            "iter 3230: loss 0.9444, time 93.33ms, mfu 3.97%\n",
            "iter 3240: loss 0.9521, time 93.19ms, mfu 3.97%\n",
            "step 3250: train loss 0.8154, val loss 1.5658\n",
            "iter 3250: loss 0.9667, time 11168.22ms, mfu 3.58%\n",
            "iter 3260: loss 0.9537, time 96.43ms, mfu 3.60%\n",
            "iter 3270: loss 0.9631, time 93.94ms, mfu 3.64%\n",
            "iter 3280: loss 0.9425, time 94.29ms, mfu 3.67%\n",
            "iter 3290: loss 0.9397, time 94.41ms, mfu 3.70%\n",
            "iter 3300: loss 0.9378, time 94.19ms, mfu 3.73%\n",
            "iter 3310: loss 0.9404, time 95.13ms, mfu 3.74%\n",
            "iter 3320: loss 0.9563, time 92.21ms, mfu 3.77%\n",
            "iter 3330: loss 0.9549, time 93.62ms, mfu 3.79%\n",
            "iter 3340: loss 0.9444, time 91.91ms, mfu 3.82%\n",
            "iter 3350: loss 0.9582, time 92.46ms, mfu 3.84%\n",
            "iter 3360: loss 0.9211, time 94.98ms, mfu 3.85%\n",
            "iter 3370: loss 0.9507, time 93.13ms, mfu 3.86%\n",
            "iter 3380: loss 0.9374, time 92.67ms, mfu 3.88%\n",
            "iter 3390: loss 0.9520, time 93.45ms, mfu 3.89%\n",
            "iter 3400: loss 0.9471, time 94.99ms, mfu 3.89%\n",
            "iter 3410: loss 0.9382, time 93.70ms, mfu 3.90%\n",
            "iter 3420: loss 0.9412, time 93.80ms, mfu 3.91%\n",
            "iter 3430: loss 0.9421, time 94.08ms, mfu 3.91%\n",
            "iter 3440: loss 0.9790, time 93.58ms, mfu 3.92%\n",
            "iter 3450: loss 0.9540, time 91.93ms, mfu 3.93%\n",
            "iter 3460: loss 0.9489, time 93.99ms, mfu 3.94%\n",
            "iter 3470: loss 0.9286, time 94.22ms, mfu 3.94%\n",
            "iter 3480: loss 0.9399, time 93.81ms, mfu 3.94%\n",
            "iter 3490: loss 0.9150, time 94.06ms, mfu 3.94%\n",
            "step 3500: train loss 0.7723, val loss 1.5758\n",
            "iter 3500: loss 0.9018, time 11112.33ms, mfu 3.55%\n",
            "iter 3510: loss 0.9139, time 91.40ms, mfu 3.61%\n",
            "iter 3520: loss 0.9112, time 91.29ms, mfu 3.65%\n",
            "iter 3530: loss 0.9379, time 93.05ms, mfu 3.69%\n",
            "iter 3540: loss 0.9211, time 90.76ms, mfu 3.73%\n",
            "iter 3550: loss 0.9201, time 93.43ms, mfu 3.76%\n",
            "iter 3560: loss 0.9461, time 91.16ms, mfu 3.79%\n",
            "iter 3570: loss 0.9356, time 91.07ms, mfu 3.82%\n",
            "iter 3580: loss 0.9247, time 92.01ms, mfu 3.84%\n",
            "iter 3590: loss 0.9138, time 91.18ms, mfu 3.87%\n",
            "iter 3600: loss 0.9255, time 93.00ms, mfu 3.88%\n",
            "iter 3610: loss 0.9118, time 93.36ms, mfu 3.89%\n",
            "iter 3620: loss 0.9110, time 92.58ms, mfu 3.91%\n",
            "iter 3630: loss 0.9080, time 91.42ms, mfu 3.92%\n",
            "iter 3640: loss 0.9069, time 93.52ms, mfu 3.93%\n",
            "iter 3650: loss 0.9095, time 91.76ms, mfu 3.94%\n",
            "iter 3660: loss 0.9248, time 94.11ms, mfu 3.94%\n",
            "iter 3670: loss 0.9260, time 95.57ms, mfu 3.94%\n",
            "iter 3680: loss 0.8974, time 92.41ms, mfu 3.95%\n",
            "iter 3690: loss 0.9276, time 94.18ms, mfu 3.95%\n",
            "iter 3700: loss 0.8679, time 93.37ms, mfu 3.95%\n",
            "iter 3710: loss 0.8727, time 93.28ms, mfu 3.96%\n",
            "iter 3720: loss 0.9012, time 91.95ms, mfu 3.97%\n",
            "iter 3730: loss 0.8980, time 93.89ms, mfu 3.97%\n",
            "iter 3740: loss 0.8897, time 91.55ms, mfu 3.98%\n",
            "step 3750: train loss 0.7337, val loss 1.6048\n",
            "iter 3750: loss 0.8912, time 11177.70ms, mfu 3.58%\n",
            "iter 3760: loss 0.9323, time 95.17ms, mfu 3.62%\n",
            "iter 3770: loss 0.9242, time 92.71ms, mfu 3.66%\n",
            "iter 3780: loss 0.9240, time 93.00ms, mfu 3.69%\n",
            "iter 3790: loss 0.8898, time 93.41ms, mfu 3.72%\n",
            "iter 3800: loss 0.9049, time 92.65ms, mfu 3.75%\n",
            "iter 3810: loss 0.9059, time 93.25ms, mfu 3.78%\n",
            "iter 3820: loss 0.8855, time 91.91ms, mfu 3.80%\n",
            "iter 3830: loss 0.8899, time 94.94ms, mfu 3.82%\n",
            "iter 3840: loss 0.8864, time 94.19ms, mfu 3.83%\n",
            "iter 3850: loss 0.8805, time 94.98ms, mfu 3.84%\n",
            "iter 3860: loss 0.8688, time 91.02ms, mfu 3.86%\n",
            "iter 3870: loss 0.8782, time 92.76ms, mfu 3.88%\n",
            "iter 3880: loss 0.8777, time 93.98ms, mfu 3.89%\n",
            "iter 3890: loss 0.8813, time 92.07ms, mfu 3.90%\n",
            "iter 3900: loss 0.8826, time 93.96ms, mfu 3.91%\n",
            "iter 3910: loss 0.8788, time 94.16ms, mfu 3.92%\n",
            "iter 3920: loss 0.8685, time 92.86ms, mfu 3.92%\n",
            "iter 3930: loss 0.8913, time 96.45ms, mfu 3.92%\n",
            "iter 3940: loss 0.8626, time 95.14ms, mfu 3.92%\n",
            "iter 3950: loss 0.8744, time 92.45ms, mfu 3.93%\n",
            "iter 3960: loss 0.9066, time 93.74ms, mfu 3.93%\n",
            "iter 3970: loss 0.8901, time 91.89ms, mfu 3.95%\n",
            "iter 3980: loss 0.8908, time 94.51ms, mfu 3.95%\n",
            "iter 3990: loss 0.8741, time 92.42ms, mfu 3.95%\n",
            "step 4000: train loss 0.7004, val loss 1.6218\n",
            "iter 4000: loss 0.8406, time 11149.95ms, mfu 3.56%\n",
            "iter 4010: loss 0.8769, time 92.41ms, mfu 3.61%\n",
            "iter 4020: loss 0.8851, time 91.68ms, mfu 3.65%\n",
            "iter 4030: loss 0.8769, time 93.47ms, mfu 3.69%\n",
            "iter 4040: loss 0.8722, time 92.54ms, mfu 3.72%\n",
            "iter 4050: loss 0.8631, time 92.05ms, mfu 3.75%\n",
            "iter 4060: loss 0.8563, time 91.46ms, mfu 3.79%\n",
            "iter 4070: loss 0.8562, time 92.67ms, mfu 3.81%\n",
            "iter 4080: loss 0.8821, time 93.14ms, mfu 3.83%\n",
            "iter 4090: loss 0.8379, time 92.19ms, mfu 3.85%\n",
            "iter 4100: loss 0.8970, time 94.04ms, mfu 3.86%\n",
            "iter 4110: loss 0.8656, time 93.58ms, mfu 3.87%\n",
            "iter 4120: loss 0.8732, time 92.18ms, mfu 3.89%\n",
            "iter 4130: loss 0.8538, time 93.33ms, mfu 3.90%\n",
            "iter 4140: loss 0.8681, time 92.94ms, mfu 3.91%\n",
            "iter 4150: loss 0.8574, time 92.29ms, mfu 3.92%\n",
            "iter 4160: loss 0.8527, time 91.72ms, mfu 3.94%\n",
            "iter 4170: loss 0.8546, time 92.25ms, mfu 3.95%\n",
            "iter 4180: loss 0.8727, time 93.31ms, mfu 3.95%\n",
            "iter 4190: loss 0.8590, time 91.36ms, mfu 3.97%\n",
            "iter 4200: loss 0.8509, time 92.05ms, mfu 3.97%\n",
            "iter 4210: loss 0.8657, time 91.01ms, mfu 3.99%\n",
            "iter 4220: loss 0.8510, time 91.75ms, mfu 3.99%\n",
            "iter 4230: loss 0.8714, time 91.45ms, mfu 4.00%\n",
            "iter 4240: loss 0.8629, time 92.52ms, mfu 4.00%\n",
            "step 4250: train loss 0.6691, val loss 1.6542\n",
            "iter 4250: loss 0.8674, time 11129.68ms, mfu 3.61%\n",
            "iter 4260: loss 0.8556, time 91.25ms, mfu 3.65%\n",
            "iter 4270: loss 0.8608, time 92.48ms, mfu 3.69%\n",
            "iter 4280: loss 0.8564, time 90.86ms, mfu 3.73%\n",
            "iter 4290: loss 0.8215, time 94.26ms, mfu 3.75%\n",
            "iter 4300: loss 0.8216, time 92.28ms, mfu 3.78%\n",
            "iter 4310: loss 0.8546, time 93.27ms, mfu 3.80%\n",
            "iter 4320: loss 0.8318, time 91.63ms, mfu 3.83%\n",
            "iter 4330: loss 0.8551, time 91.50ms, mfu 3.85%\n",
            "iter 4340: loss 0.8255, time 91.54ms, mfu 3.88%\n",
            "iter 4350: loss 0.8311, time 93.72ms, mfu 3.89%\n",
            "iter 4360: loss 0.8603, time 94.39ms, mfu 3.89%\n",
            "iter 4370: loss 0.8514, time 92.52ms, mfu 3.91%\n",
            "iter 4380: loss 0.8398, time 94.30ms, mfu 3.91%\n",
            "iter 4390: loss 0.8578, time 89.18ms, mfu 3.94%\n",
            "iter 4400: loss 0.8387, time 94.08ms, mfu 3.94%\n",
            "iter 4410: loss 0.8480, time 91.49ms, mfu 3.95%\n",
            "iter 4420: loss 0.8520, time 92.22ms, mfu 3.96%\n",
            "iter 4430: loss 0.8405, time 92.26ms, mfu 3.97%\n",
            "iter 4440: loss 0.8329, time 95.65ms, mfu 3.96%\n",
            "iter 4450: loss 0.8398, time 94.32ms, mfu 3.96%\n",
            "iter 4460: loss 0.8286, time 93.87ms, mfu 3.96%\n",
            "iter 4470: loss 0.8488, time 93.07ms, mfu 3.97%\n",
            "iter 4480: loss 0.8228, time 93.16ms, mfu 3.97%\n",
            "iter 4490: loss 0.8367, time 90.94ms, mfu 3.98%\n",
            "step 4500: train loss 0.6445, val loss 1.6688\n",
            "iter 4500: loss 0.8556, time 11145.39ms, mfu 3.59%\n",
            "iter 4510: loss 0.8452, time 93.97ms, mfu 3.63%\n",
            "iter 4520: loss 0.8263, time 94.01ms, mfu 3.66%\n",
            "iter 4530: loss 0.8460, time 93.02ms, mfu 3.69%\n",
            "iter 4540: loss 0.8436, time 93.34ms, mfu 3.72%\n",
            "iter 4550: loss 0.8698, time 91.45ms, mfu 3.76%\n",
            "iter 4560: loss 0.8305, time 92.27ms, mfu 3.79%\n",
            "iter 4570: loss 0.8390, time 91.29ms, mfu 3.82%\n",
            "iter 4580: loss 0.8467, time 93.13ms, mfu 3.83%\n",
            "iter 4590: loss 0.8503, time 90.86ms, mfu 3.86%\n",
            "iter 4600: loss 0.8134, time 93.72ms, mfu 3.87%\n",
            "iter 4610: loss 0.8583, time 91.66ms, mfu 3.89%\n",
            "iter 4620: loss 0.8268, time 92.94ms, mfu 3.90%\n",
            "iter 4630: loss 0.8133, time 91.04ms, mfu 3.92%\n",
            "iter 4640: loss 0.8336, time 94.87ms, mfu 3.92%\n",
            "iter 4650: loss 0.8545, time 92.70ms, mfu 3.93%\n",
            "iter 4660: loss 0.8400, time 92.70ms, mfu 3.94%\n",
            "iter 4670: loss 0.8306, time 93.38ms, mfu 3.95%\n",
            "iter 4680: loss 0.8520, time 92.70ms, mfu 3.95%\n",
            "iter 4690: loss 0.8404, time 92.77ms, mfu 3.96%\n",
            "iter 4700: loss 0.8110, time 92.47ms, mfu 3.97%\n",
            "iter 4710: loss 0.7855, time 90.78ms, mfu 3.98%\n",
            "iter 4720: loss 0.8230, time 91.76ms, mfu 3.99%\n",
            "iter 4730: loss 0.8112, time 90.85ms, mfu 4.00%\n",
            "iter 4740: loss 0.8190, time 90.32ms, mfu 4.01%\n",
            "step 4750: train loss 0.6279, val loss 1.6858\n",
            "iter 4750: loss 0.7996, time 11176.54ms, mfu 3.61%\n",
            "iter 4760: loss 0.8070, time 94.21ms, mfu 3.65%\n",
            "iter 4770: loss 0.7922, time 96.65ms, mfu 3.67%\n",
            "iter 4780: loss 0.8034, time 92.66ms, mfu 3.70%\n",
            "iter 4790: loss 0.8258, time 93.09ms, mfu 3.73%\n",
            "iter 4800: loss 0.8132, time 92.52ms, mfu 3.76%\n",
            "iter 4810: loss 0.8350, time 95.37ms, mfu 3.78%\n",
            "iter 4820: loss 0.8080, time 93.21ms, mfu 3.80%\n",
            "iter 4830: loss 0.8146, time 92.95ms, mfu 3.82%\n",
            "iter 4840: loss 0.8162, time 92.39ms, mfu 3.84%\n",
            "iter 4850: loss 0.8191, time 92.78ms, mfu 3.86%\n",
            "iter 4860: loss 0.8182, time 94.36ms, mfu 3.87%\n",
            "iter 4870: loss 0.7985, time 96.72ms, mfu 3.87%\n",
            "iter 4880: loss 0.8228, time 95.17ms, mfu 3.87%\n",
            "iter 4890: loss 0.8014, time 93.62ms, mfu 3.88%\n",
            "iter 4900: loss 0.8086, time 91.85ms, mfu 3.90%\n",
            "iter 4910: loss 0.8302, time 92.71ms, mfu 3.91%\n",
            "iter 4920: loss 0.8136, time 93.96ms, mfu 3.92%\n",
            "iter 4930: loss 0.7977, time 92.41ms, mfu 3.93%\n",
            "iter 4940: loss 0.7843, time 93.75ms, mfu 3.93%\n",
            "iter 4950: loss 0.8221, time 94.62ms, mfu 3.93%\n",
            "iter 4960: loss 0.8218, time 94.64ms, mfu 3.93%\n",
            "iter 4970: loss 0.7801, time 93.84ms, mfu 3.94%\n",
            "iter 4980: loss 0.7946, time 94.05ms, mfu 3.94%\n",
            "iter 4990: loss 0.8226, time 94.72ms, mfu 3.94%\n",
            "step 5000: train loss 0.6130, val loss 1.7054\n",
            "iter 5000: loss 0.8122, time 11176.11ms, mfu 3.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJFdXOli6XjW",
        "outputId": "3547c549-a73d-4bc4-f069-3172dbd248fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And come, my lord,\n",
            "Who shall be your hands to my country's prayers?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Now, enown to your noble majesty\n",
            "Yourself from her to strew the beggars.\n",
            "\n",
            "ISABELLA:\n",
            "Your dreams are not like your ways!\n",
            "\n",
            "ANGELO:\n",
            "The tents you have stuff'd i' the years of men\n",
            "But a sign of the law, to the laon,--\n",
            "And the issue of the other sweet precials!--\n",
            "As for the king thrust for the ground:\n",
            "Ere I know the fight of death? and though as the hand,\n",
            "As being as the hung courts of night-blood\n",
            "And follow\n",
            "---------------\n",
            "\n",
            "Menenius, I must rest be true;\n",
            "And I have seen and for weak threatens of their son,\n",
            "To the dead of courts makes at mine.\n",
            "There is not perjured his pardon; and to make him\n",
            "To the place of the same shall be graced on the golden,\n",
            "To the good of her eyes of her children to the crown:\n",
            "And so love I may make the world's child,\n",
            "To see it bring the hands of God's wife,\n",
            "And to have me under ornamented by their royaltish;\n",
            "And so there we must send the crown,\n",
            "For the death of the other in the flower.\n",
            "\n",
            "GLOU\n",
            "---------------\n",
            "\n",
            "Men that I beseech your husband; go with me,\n",
            "To the princes of English courts, and myself,\n",
            "Set the Earl of Lancaster, to our gracious son,\n",
            "If two be revenged with I have a cause\n",
            "Of that incexation of this will to the people\n",
            "Of two nature have eviled been bastardy displease\n",
            "That wringly springs forth the second of the truth\n",
            "That which they be not short to be brief;\n",
            "For the substance should be to put by the power.\n",
            "\n",
            "CORIOLANUS:\n",
            "Nay, we will not summer it.\n",
            "\n",
            "CORIOLANUS:\n",
            "If thou wert as a raw of thee\n",
            "\n",
            "---------------\n",
            "\n",
            "The sension of hour itself and gives;\n",
            "And whether virtue could not sue of when\n",
            "More pitrimage of the world's rage makes your\n",
            "foes, he will not seem array upon the wars.\n",
            "I am in his last head, more than he hath seen; and\n",
            "her borne the waste, and with his corruption with a tiger\n",
            "woman to her accustomation. To all the growth of\n",
            "his like a life a shook for a bitter cause; which owe answer thy prize,\n",
            "that I would be ta'en the comfort, a story and a\n",
            "goodly dead nor fool. The sun who first thou a \n",
            "appr\n",
            "---------------\n",
            "\n",
            "That lamb Nell, the queen's party with light:\n",
            "So doth go to palt your subjects to your hands\n",
            "And do you burn as well the lamb of this eyes?\n",
            "The father regal hath with mine own witness death:\n",
            "Her own my heart's present from me that ne'er made me,\n",
            "As here doth champion with many estate.\n",
            "As a sister wife, and many is not made the queen.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Methinks a mother, that the blood of the sap\n",
            "that lies her owe should move but his highway of it!\n",
            "\n",
            "DUKE OF YORK:\n",
            "This is full of English warms. No\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He hath he stain'd him to the gentleman to his\n",
            "A peace of his penitence, he was wing, at a\n",
            "town and overtor: boy, were he brought it not\n",
            "To take him so truth to a blasting. But it goes,\n",
            "To lie and o'erthrough it out of me a sir\n",
            "And break for a mistress,--\n",
            "Till the one, here be my hand, what he would Back's,--\n",
            "In God's lance, and, till I never content:\n",
            "To me, be your lady's son,--\n",
            "\n",
            "Shepherd:\n",
            "Ay, let me know your grace and so much to be gone.\n",
            "\n",
            "ROMEO:\n",
            "You shall, he is so hire, and like t\n",
            "---------------\n",
            "\n",
            "Shall I see?\n",
            "\n",
            "Shepherd:\n",
            "But she was made, when in a shrewd, if my breast a\n",
            "show inch a thing. An excusation main it is\n",
            "bried to the curst world, and have you in God have of Blandine.\n",
            "\n",
            "Provost:\n",
            "Her eyes, and fair levishes marriages not of you: give me the\n",
            "charge, or not young assisting from this is languished blood. Your\n",
            "pension was slainly as fair, as you'll have some willie\n",
            "to pray it and against the controdion of a\n",
            "noble former, and that I have to bear the gap: to it\n",
            "will look your purpose to \n",
            "---------------\n",
            "\n",
            "I have heard all his enemy or will not shrift\n",
            "Upon his sight prophecial, he shall be his\n",
            "As you and bolded and vile your power instruments.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "\n",
            "ESCALUS:\n",
            "Surely well-groan'd in this.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Ay, to be a man of love to him so hot your\n",
            "pardon his free encounter to stir you; but now\n",
            "I will truly so a well of proud tongue to speak with the hand,\n",
            "And thou and make the happy troubled days.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Yes, stay no more of that change from off.\n",
            "He could you must not have my p\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The realm of thy husband,\n",
            "To be for a molehy brother of thine;\n",
            "Having breathed fire so sweathed as mine.\n",
            "And sleeping shall be not so do to the lords\n",
            "Of such a man of his power?\n",
            "But yet this is but her my soul, is not mannerr'd there\n",
            "As he seems me that I must tell my injuries,\n",
            "When I have said of hatred as man's does as if\n",
            "The duke of the petty services of his innocent,\n",
            "And sound begins to his push a goodly relished by-his face,\n",
            "And what I can understand ruin him.\n",
            "\n",
            "CORIOLANUS:\n",
            "Why, si\n",
            "---------------\n",
            "\n",
            "Here's good and dream:\n",
            "I am three and so still our charity.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "My lord, I warrant the times of my loving soul!\n",
            "\n",
            "KING RICHARD III:\n",
            "Well, we have not seen to Ravenspurgh Hereford!\n",
            "That I shall we speak of thy south.\n",
            "\n",
            "KING RICHARD II:\n",
            "Then see this a bound, I plant--\n",
            "Why, you will not stay them in the house of York?\n",
            "\n",
            "CATESBY:\n",
            "Well, Warwick, that shall say the bones,\n",
            "Wherefore the infliction that should but the father\n",
            "And come to the place of York.\n",
            "\n",
            "KING RICHARD III:\n",
            "What is't my re\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Some training runs exceed 10 mins hence set the max_iters at 3000:"
      ],
      "metadata": {
        "id": "AKmV4y9kVeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --max_iters=3000 --n_layer=2 --n_head=5 --n_embd=320 --dtype=float16"
      ],
      "metadata": {
        "id": "Gf2lhqYc8c-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87150ab9-c23e-45be-b87c-5ee47d2f5098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: n_layer = 2\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 320\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 2.48M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 10, with 2,560,320 parameters\n",
            "num non-decayed parameter tensors: 5, with 1,600 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0427 06:31:52.680000 2837 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.2615, val loss 4.2566\n",
            "iter 0: loss 4.2628, time 35313.03ms, mfu -100.00%\n",
            "iter 10: loss 3.4211, time 6.91ms, mfu 12.81%\n",
            "iter 20: loss 3.0322, time 6.92ms, mfu 12.81%\n",
            "iter 30: loss 2.7275, time 7.10ms, mfu 12.77%\n",
            "iter 40: loss 2.6110, time 6.72ms, mfu 12.81%\n",
            "iter 50: loss 2.5552, time 6.88ms, mfu 12.82%\n",
            "iter 60: loss 2.5324, time 6.66ms, mfu 12.86%\n",
            "iter 70: loss 2.5133, time 6.78ms, mfu 12.88%\n",
            "iter 80: loss 2.4910, time 6.78ms, mfu 12.90%\n",
            "iter 90: loss 2.5080, time 6.87ms, mfu 12.90%\n",
            "iter 100: loss 2.4856, time 6.83ms, mfu 12.90%\n",
            "iter 110: loss 2.4829, time 6.65ms, mfu 12.94%\n",
            "iter 120: loss 2.4619, time 6.67ms, mfu 12.97%\n",
            "iter 130: loss 2.4609, time 7.59ms, mfu 12.84%\n",
            "iter 140: loss 2.4389, time 6.89ms, mfu 12.84%\n",
            "iter 150: loss 2.4231, time 6.91ms, mfu 12.84%\n",
            "iter 160: loss 2.4400, time 8.43ms, mfu 12.60%\n",
            "iter 170: loss 2.4141, time 7.31ms, mfu 12.55%\n",
            "iter 180: loss 2.4012, time 6.86ms, mfu 12.59%\n",
            "iter 190: loss 2.3658, time 6.91ms, mfu 12.61%\n",
            "iter 200: loss 2.3477, time 6.59ms, mfu 12.69%\n",
            "iter 210: loss 2.3314, time 9.80ms, mfu 12.32%\n",
            "iter 220: loss 2.3262, time 11.98ms, mfu 11.83%\n",
            "iter 230: loss 2.3030, time 9.62ms, mfu 11.57%\n",
            "iter 240: loss 2.2734, time 8.86ms, mfu 11.41%\n",
            "step 250: train loss 2.1470, val loss 2.2190\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.2470, time 3524.84ms, mfu 10.27%\n",
            "iter 260: loss 2.2046, time 7.03ms, mfu 10.50%\n",
            "iter 270: loss 2.1646, time 7.31ms, mfu 10.66%\n",
            "iter 280: loss 2.1084, time 7.00ms, mfu 10.86%\n",
            "iter 290: loss 2.1084, time 7.16ms, mfu 11.01%\n",
            "iter 300: loss 2.0660, time 6.68ms, mfu 11.23%\n",
            "iter 310: loss 2.0170, time 6.62ms, mfu 11.44%\n",
            "iter 320: loss 2.0144, time 7.06ms, mfu 11.55%\n",
            "iter 330: loss 1.9667, time 6.79ms, mfu 11.70%\n",
            "iter 340: loss 1.9776, time 10.31ms, mfu 11.39%\n",
            "iter 350: loss 1.9483, time 7.28ms, mfu 11.47%\n",
            "iter 360: loss 1.9448, time 6.58ms, mfu 11.66%\n",
            "iter 370: loss 1.9034, time 6.80ms, mfu 11.80%\n",
            "iter 380: loss 1.8899, time 6.84ms, mfu 11.91%\n",
            "iter 390: loss 1.8696, time 6.80ms, mfu 12.02%\n",
            "iter 400: loss 1.8767, time 6.93ms, mfu 12.10%\n",
            "iter 410: loss 1.8692, time 6.80ms, mfu 12.19%\n",
            "iter 420: loss 1.8377, time 7.15ms, mfu 12.21%\n",
            "iter 430: loss 1.8068, time 6.78ms, mfu 12.29%\n",
            "iter 440: loss 1.8185, time 6.65ms, mfu 12.39%\n",
            "iter 450: loss 1.8190, time 6.94ms, mfu 12.43%\n",
            "iter 460: loss 1.8085, time 6.83ms, mfu 12.48%\n",
            "iter 470: loss 1.7595, time 6.69ms, mfu 12.55%\n",
            "iter 480: loss 1.7367, time 7.01ms, mfu 12.56%\n",
            "iter 490: loss 1.7605, time 6.84ms, mfu 12.60%\n",
            "step 500: train loss 1.6420, val loss 1.8458\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.7603, time 3693.46ms, mfu 11.34%\n",
            "iter 510: loss 1.7370, time 13.36ms, mfu 10.87%\n",
            "iter 520: loss 1.7617, time 7.07ms, mfu 11.03%\n",
            "iter 530: loss 1.7591, time 11.02ms, mfu 10.73%\n",
            "iter 540: loss 1.7258, time 6.78ms, mfu 10.96%\n",
            "iter 550: loss 1.7116, time 6.86ms, mfu 11.16%\n",
            "iter 560: loss 1.6982, time 7.03ms, mfu 11.30%\n",
            "iter 570: loss 1.6773, time 6.79ms, mfu 11.47%\n",
            "iter 580: loss 1.7185, time 7.87ms, mfu 11.45%\n",
            "iter 590: loss 1.6872, time 7.11ms, mfu 11.55%\n",
            "iter 600: loss 1.6919, time 6.64ms, mfu 11.73%\n",
            "iter 610: loss 1.6660, time 7.27ms, mfu 11.77%\n",
            "iter 620: loss 1.6717, time 7.16ms, mfu 11.83%\n",
            "iter 630: loss 1.6700, time 6.88ms, mfu 11.93%\n",
            "iter 640: loss 1.6298, time 6.68ms, mfu 12.06%\n",
            "iter 650: loss 1.6503, time 6.72ms, mfu 12.17%\n",
            "iter 660: loss 1.6572, time 6.83ms, mfu 12.25%\n",
            "iter 670: loss 1.6419, time 7.20ms, mfu 12.26%\n",
            "iter 680: loss 1.6605, time 7.90ms, mfu 12.15%\n",
            "iter 690: loss 1.6497, time 6.78ms, mfu 12.24%\n",
            "iter 700: loss 1.6480, time 11.56ms, mfu 11.78%\n",
            "iter 710: loss 1.6204, time 6.85ms, mfu 11.89%\n",
            "iter 720: loss 1.6303, time 7.02ms, mfu 11.97%\n",
            "iter 730: loss 1.5985, time 6.83ms, mfu 12.07%\n",
            "iter 740: loss 1.6357, time 6.65ms, mfu 12.19%\n",
            "step 750: train loss 1.4917, val loss 1.6846\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.6315, time 3466.43ms, mfu 10.97%\n",
            "iter 760: loss 1.6189, time 6.63ms, mfu 11.21%\n",
            "iter 770: loss 1.5993, time 6.73ms, mfu 11.40%\n",
            "iter 780: loss 1.5810, time 6.85ms, mfu 11.56%\n",
            "iter 790: loss 1.6235, time 6.64ms, mfu 11.73%\n",
            "iter 800: loss 1.5544, time 11.72ms, mfu 11.31%\n",
            "iter 810: loss 1.5982, time 8.99ms, mfu 11.17%\n",
            "iter 820: loss 1.5320, time 9.84ms, mfu 10.95%\n",
            "iter 830: loss 1.5730, time 9.01ms, mfu 10.84%\n",
            "iter 840: loss 1.5400, time 8.57ms, mfu 10.79%\n",
            "iter 850: loss 1.5898, time 8.55ms, mfu 10.74%\n",
            "iter 860: loss 1.5922, time 10.05ms, mfu 10.55%\n",
            "iter 870: loss 1.5722, time 10.33ms, mfu 10.35%\n",
            "iter 880: loss 1.5773, time 10.68ms, mfu 10.14%\n",
            "iter 890: loss 1.5583, time 7.02ms, mfu 10.39%\n",
            "iter 900: loss 1.5645, time 6.75ms, mfu 10.66%\n",
            "iter 910: loss 1.5593, time 6.59ms, mfu 10.94%\n",
            "iter 920: loss 1.5576, time 6.62ms, mfu 11.18%\n",
            "iter 930: loss 1.5675, time 6.77ms, mfu 11.37%\n",
            "iter 940: loss 1.5998, time 7.13ms, mfu 11.47%\n",
            "iter 950: loss 1.5496, time 7.62ms, mfu 11.49%\n",
            "iter 960: loss 1.5047, time 7.12ms, mfu 11.58%\n",
            "iter 970: loss 1.5452, time 6.90ms, mfu 11.71%\n",
            "iter 980: loss 1.5343, time 6.76ms, mfu 11.84%\n",
            "iter 990: loss 1.5305, time 6.72ms, mfu 11.98%\n",
            "step 1000: train loss 1.4157, val loss 1.6230\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.5585, time 3487.06ms, mfu 10.78%\n",
            "iter 1010: loss 1.5417, time 6.69ms, mfu 11.03%\n",
            "iter 1020: loss 1.5312, time 7.09ms, mfu 11.17%\n",
            "iter 1030: loss 1.4997, time 6.76ms, mfu 11.36%\n",
            "iter 1040: loss 1.5370, time 6.55ms, mfu 11.58%\n",
            "iter 1050: loss 1.5381, time 6.89ms, mfu 11.70%\n",
            "iter 1060: loss 1.5159, time 6.84ms, mfu 11.83%\n",
            "iter 1070: loss 1.5396, time 7.08ms, mfu 11.89%\n",
            "iter 1080: loss 1.5175, time 6.69ms, mfu 12.03%\n",
            "iter 1090: loss 1.4993, time 6.93ms, mfu 12.10%\n",
            "iter 1100: loss 1.5181, time 6.67ms, mfu 12.22%\n",
            "iter 1110: loss 1.5078, time 7.05ms, mfu 12.25%\n",
            "iter 1120: loss 1.4926, time 6.78ms, mfu 12.33%\n",
            "iter 1130: loss 1.4967, time 6.64ms, mfu 12.43%\n",
            "iter 1140: loss 1.4913, time 6.77ms, mfu 12.49%\n",
            "iter 1150: loss 1.5007, time 6.74ms, mfu 12.56%\n",
            "iter 1160: loss 1.4991, time 10.75ms, mfu 12.12%\n",
            "iter 1170: loss 1.5109, time 9.25ms, mfu 11.87%\n",
            "iter 1180: loss 1.5036, time 9.06ms, mfu 11.66%\n",
            "iter 1190: loss 1.4829, time 8.97ms, mfu 11.48%\n",
            "iter 1200: loss 1.4920, time 8.67ms, mfu 11.35%\n",
            "iter 1210: loss 1.4969, time 8.66ms, mfu 11.24%\n",
            "iter 1220: loss 1.4735, time 8.82ms, mfu 11.12%\n",
            "iter 1230: loss 1.4989, time 10.21ms, mfu 10.87%\n",
            "iter 1240: loss 1.4875, time 10.33ms, mfu 10.64%\n",
            "step 1250: train loss 1.3705, val loss 1.6015\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.4809, time 3475.71ms, mfu 9.58%\n",
            "iter 1260: loss 1.4674, time 6.90ms, mfu 9.90%\n",
            "iter 1270: loss 1.5169, time 7.55ms, mfu 10.08%\n",
            "iter 1280: loss 1.4760, time 7.08ms, mfu 10.33%\n",
            "iter 1290: loss 1.4814, time 6.73ms, mfu 10.61%\n",
            "iter 1300: loss 1.4682, time 6.88ms, mfu 10.83%\n",
            "iter 1310: loss 1.4418, time 7.44ms, mfu 10.94%\n",
            "iter 1320: loss 1.4610, time 6.87ms, mfu 11.13%\n",
            "iter 1330: loss 1.4836, time 7.46ms, mfu 11.20%\n",
            "iter 1340: loss 1.4670, time 6.99ms, mfu 11.35%\n",
            "iter 1350: loss 1.4707, time 6.83ms, mfu 11.51%\n",
            "iter 1360: loss 1.4636, time 7.51ms, mfu 11.54%\n",
            "iter 1370: loss 1.4837, time 7.02ms, mfu 11.64%\n",
            "iter 1380: loss 1.4606, time 6.81ms, mfu 11.78%\n",
            "iter 1390: loss 1.5119, time 6.76ms, mfu 11.91%\n",
            "iter 1400: loss 1.5071, time 7.34ms, mfu 11.92%\n",
            "iter 1410: loss 1.4710, time 6.83ms, mfu 12.02%\n",
            "iter 1420: loss 1.4773, time 6.74ms, mfu 12.13%\n",
            "iter 1430: loss 1.4468, time 6.90ms, mfu 12.20%\n",
            "iter 1440: loss 1.4609, time 7.78ms, mfu 12.12%\n",
            "iter 1450: loss 1.4789, time 6.77ms, mfu 12.21%\n",
            "iter 1460: loss 1.4666, time 7.05ms, mfu 12.25%\n",
            "iter 1470: loss 1.4618, time 6.93ms, mfu 12.30%\n",
            "iter 1480: loss 1.4778, time 6.86ms, mfu 12.36%\n",
            "iter 1490: loss 1.4606, time 6.70ms, mfu 12.44%\n",
            "step 1500: train loss 1.3411, val loss 1.5809\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.4310, time 3757.10ms, mfu 11.20%\n",
            "iter 1510: loss 1.4698, time 7.14ms, mfu 11.32%\n",
            "iter 1520: loss 1.4853, time 6.96ms, mfu 11.46%\n",
            "iter 1530: loss 1.4872, time 6.84ms, mfu 11.61%\n",
            "iter 1540: loss 1.4858, time 7.62ms, mfu 11.61%\n",
            "iter 1550: loss 1.4790, time 7.63ms, mfu 11.61%\n",
            "iter 1560: loss 1.4499, time 6.78ms, mfu 11.75%\n",
            "iter 1570: loss 1.4871, time 7.04ms, mfu 11.83%\n",
            "iter 1580: loss 1.4625, time 7.37ms, mfu 11.85%\n",
            "iter 1590: loss 1.4353, time 10.79ms, mfu 11.48%\n",
            "iter 1600: loss 1.4322, time 7.33ms, mfu 11.54%\n",
            "iter 1610: loss 1.4709, time 6.99ms, mfu 11.65%\n",
            "iter 1620: loss 1.4518, time 7.20ms, mfu 11.72%\n",
            "iter 1630: loss 1.4556, time 6.70ms, mfu 11.87%\n",
            "iter 1640: loss 1.4407, time 6.63ms, mfu 12.01%\n",
            "iter 1650: loss 1.4459, time 7.01ms, mfu 12.07%\n",
            "iter 1660: loss 1.4156, time 7.84ms, mfu 12.00%\n",
            "iter 1670: loss 1.4297, time 6.58ms, mfu 12.14%\n",
            "iter 1680: loss 1.4648, time 7.02ms, mfu 12.19%\n",
            "iter 1690: loss 1.4620, time 6.73ms, mfu 12.28%\n",
            "iter 1700: loss 1.4418, time 8.38ms, mfu 12.11%\n",
            "iter 1710: loss 1.4624, time 6.89ms, mfu 12.18%\n",
            "iter 1720: loss 1.4162, time 6.87ms, mfu 12.25%\n",
            "iter 1730: loss 1.4504, time 7.00ms, mfu 12.29%\n",
            "iter 1740: loss 1.4623, time 8.15ms, mfu 12.15%\n",
            "step 1750: train loss 1.3121, val loss 1.5584\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.4621, time 3596.28ms, mfu 10.94%\n",
            "iter 1760: loss 1.4132, time 12.26ms, mfu 10.56%\n",
            "iter 1770: loss 1.4330, time 8.55ms, mfu 10.54%\n",
            "iter 1780: loss 1.4363, time 8.77ms, mfu 10.50%\n",
            "iter 1790: loss 1.4243, time 11.20ms, mfu 10.24%\n",
            "iter 1800: loss 1.4568, time 10.64ms, mfu 10.04%\n",
            "iter 1810: loss 1.4287, time 10.77ms, mfu 9.86%\n",
            "iter 1820: loss 1.4290, time 6.79ms, mfu 10.18%\n",
            "iter 1830: loss 1.4280, time 6.90ms, mfu 10.44%\n",
            "iter 1840: loss 1.4531, time 6.69ms, mfu 10.72%\n",
            "iter 1850: loss 1.3984, time 6.67ms, mfu 10.98%\n",
            "iter 1860: loss 1.4367, time 6.82ms, mfu 11.17%\n",
            "iter 1870: loss 1.4074, time 11.28ms, mfu 10.84%\n",
            "iter 1880: loss 1.4407, time 7.12ms, mfu 11.00%\n",
            "iter 1890: loss 1.4301, time 7.39ms, mfu 11.10%\n",
            "iter 1900: loss 1.4530, time 6.67ms, mfu 11.31%\n",
            "iter 1910: loss 1.4367, time 6.76ms, mfu 11.49%\n",
            "iter 1920: loss 1.4126, time 7.65ms, mfu 11.50%\n",
            "iter 1930: loss 1.4029, time 7.17ms, mfu 11.58%\n",
            "iter 1940: loss 1.4347, time 6.65ms, mfu 11.75%\n",
            "iter 1950: loss 1.3908, time 6.65ms, mfu 11.91%\n",
            "iter 1960: loss 1.4170, time 6.60ms, mfu 12.06%\n",
            "iter 1970: loss 1.4281, time 6.91ms, mfu 12.13%\n",
            "iter 1980: loss 1.3859, time 6.60ms, mfu 12.26%\n",
            "iter 1990: loss 1.4110, time 6.84ms, mfu 12.32%\n",
            "step 2000: train loss 1.2921, val loss 1.5528\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.4265, time 3518.96ms, mfu 11.09%\n",
            "iter 2010: loss 1.3840, time 6.78ms, mfu 11.29%\n",
            "iter 2020: loss 1.3683, time 6.87ms, mfu 11.45%\n",
            "iter 2030: loss 1.4395, time 6.72ms, mfu 11.62%\n",
            "iter 2040: loss 1.3802, time 6.91ms, mfu 11.74%\n",
            "iter 2050: loss 1.4119, time 6.68ms, mfu 11.89%\n",
            "iter 2060: loss 1.4137, time 6.80ms, mfu 12.00%\n",
            "iter 2070: loss 1.3867, time 7.02ms, mfu 12.06%\n",
            "iter 2080: loss 1.4164, time 10.51ms, mfu 11.70%\n",
            "iter 2090: loss 1.4297, time 8.62ms, mfu 11.55%\n",
            "iter 2100: loss 1.4141, time 10.45ms, mfu 11.24%\n",
            "iter 2110: loss 1.3877, time 8.83ms, mfu 11.12%\n",
            "iter 2120: loss 1.3647, time 8.77ms, mfu 11.02%\n",
            "iter 2130: loss 1.3888, time 11.55ms, mfu 10.68%\n",
            "iter 2140: loss 1.3971, time 10.51ms, mfu 10.46%\n",
            "iter 2150: loss 1.3949, time 10.74ms, mfu 10.23%\n",
            "iter 2160: loss 1.3705, time 10.33ms, mfu 10.07%\n",
            "iter 2170: loss 1.4212, time 6.65ms, mfu 10.39%\n",
            "iter 2180: loss 1.3731, time 7.14ms, mfu 10.59%\n",
            "iter 2190: loss 1.4283, time 6.86ms, mfu 10.82%\n",
            "iter 2200: loss 1.4041, time 7.01ms, mfu 11.00%\n",
            "iter 2210: loss 1.3963, time 6.96ms, mfu 11.17%\n",
            "iter 2220: loss 1.4227, time 8.60ms, mfu 11.08%\n",
            "iter 2230: loss 1.3623, time 6.81ms, mfu 11.27%\n",
            "iter 2240: loss 1.4378, time 7.37ms, mfu 11.35%\n",
            "step 2250: train loss 1.2787, val loss 1.5472\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.3822, time 3566.48ms, mfu 10.21%\n",
            "iter 2260: loss 1.3666, time 6.83ms, mfu 10.49%\n",
            "iter 2270: loss 1.3765, time 6.70ms, mfu 10.76%\n",
            "iter 2280: loss 1.3906, time 7.19ms, mfu 10.91%\n",
            "iter 2290: loss 1.3865, time 6.80ms, mfu 11.12%\n",
            "iter 2300: loss 1.4007, time 6.74ms, mfu 11.32%\n",
            "iter 2310: loss 1.3897, time 6.91ms, mfu 11.47%\n",
            "iter 2320: loss 1.3885, time 7.52ms, mfu 11.50%\n",
            "iter 2330: loss 1.4063, time 8.37ms, mfu 11.41%\n",
            "iter 2340: loss 1.3864, time 6.93ms, mfu 11.54%\n",
            "iter 2350: loss 1.3848, time 6.66ms, mfu 11.72%\n",
            "iter 2360: loss 1.3720, time 7.01ms, mfu 11.81%\n",
            "iter 2370: loss 1.4060, time 7.36ms, mfu 11.83%\n",
            "iter 2380: loss 1.3702, time 6.87ms, mfu 11.93%\n",
            "iter 2390: loss 1.3751, time 6.80ms, mfu 12.04%\n",
            "iter 2400: loss 1.3941, time 6.93ms, mfu 12.11%\n",
            "iter 2410: loss 1.3859, time 6.59ms, mfu 12.24%\n",
            "iter 2420: loss 1.3864, time 8.43ms, mfu 12.07%\n",
            "iter 2430: loss 1.3682, time 8.89ms, mfu 11.86%\n",
            "iter 2440: loss 1.4001, time 9.05ms, mfu 11.65%\n",
            "iter 2450: loss 1.4261, time 8.61ms, mfu 11.51%\n",
            "iter 2460: loss 1.3935, time 8.63ms, mfu 11.39%\n",
            "iter 2470: loss 1.3912, time 10.61ms, mfu 11.08%\n",
            "iter 2480: loss 1.3744, time 9.08ms, mfu 10.95%\n",
            "iter 2490: loss 1.3915, time 10.66ms, mfu 10.68%\n",
            "step 2500: train loss 1.2577, val loss 1.5316\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.4131, time 3606.20ms, mfu 9.62%\n",
            "iter 2510: loss 1.4054, time 7.03ms, mfu 9.91%\n",
            "iter 2520: loss 1.3921, time 6.86ms, mfu 10.21%\n",
            "iter 2530: loss 1.3921, time 7.16ms, mfu 10.42%\n",
            "iter 2540: loss 1.3996, time 6.65ms, mfu 10.71%\n",
            "iter 2550: loss 1.3787, time 6.79ms, mfu 10.95%\n",
            "iter 2560: loss 1.3951, time 6.82ms, mfu 11.15%\n",
            "iter 2570: loss 1.3638, time 6.90ms, mfu 11.32%\n",
            "iter 2580: loss 1.3668, time 6.53ms, mfu 11.54%\n",
            "iter 2590: loss 1.3602, time 11.77ms, mfu 11.14%\n",
            "iter 2600: loss 1.3719, time 7.18ms, mfu 11.26%\n",
            "iter 2610: loss 1.3781, time 7.16ms, mfu 11.37%\n",
            "iter 2620: loss 1.3501, time 8.07ms, mfu 11.32%\n",
            "iter 2630: loss 1.3717, time 7.04ms, mfu 11.45%\n",
            "iter 2640: loss 1.3753, time 7.04ms, mfu 11.56%\n",
            "iter 2650: loss 1.4044, time 6.83ms, mfu 11.70%\n",
            "iter 2660: loss 1.3589, time 6.68ms, mfu 11.86%\n",
            "iter 2670: loss 1.3932, time 7.04ms, mfu 11.93%\n",
            "iter 2680: loss 1.3602, time 6.82ms, mfu 12.03%\n",
            "iter 2690: loss 1.3535, time 6.88ms, mfu 12.11%\n",
            "iter 2700: loss 1.3896, time 6.96ms, mfu 12.17%\n",
            "iter 2710: loss 1.3810, time 7.10ms, mfu 12.20%\n",
            "iter 2720: loss 1.3612, time 6.97ms, mfu 12.25%\n",
            "iter 2730: loss 1.3836, time 6.77ms, mfu 12.33%\n",
            "iter 2740: loss 1.3826, time 11.53ms, mfu 11.87%\n",
            "step 2750: train loss 1.2476, val loss 1.5222\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.3945, time 3830.39ms, mfu 10.68%\n",
            "iter 2760: loss 1.3536, time 6.72ms, mfu 10.93%\n",
            "iter 2770: loss 1.3505, time 6.68ms, mfu 11.16%\n",
            "iter 2780: loss 1.3526, time 6.75ms, mfu 11.36%\n",
            "iter 2790: loss 1.3393, time 11.57ms, mfu 10.99%\n",
            "iter 2800: loss 1.3403, time 6.80ms, mfu 11.19%\n",
            "iter 2810: loss 1.3580, time 7.28ms, mfu 11.28%\n",
            "iter 2820: loss 1.3379, time 6.74ms, mfu 11.47%\n",
            "iter 2830: loss 1.3810, time 6.79ms, mfu 11.63%\n",
            "iter 2840: loss 1.3742, time 7.85ms, mfu 11.59%\n",
            "iter 2850: loss 1.3733, time 6.62ms, mfu 11.77%\n",
            "iter 2860: loss 1.3422, time 7.17ms, mfu 11.82%\n",
            "iter 2870: loss 1.3678, time 7.08ms, mfu 11.89%\n",
            "iter 2880: loss 1.3703, time 7.14ms, mfu 11.94%\n",
            "iter 2890: loss 1.3550, time 6.73ms, mfu 12.06%\n",
            "iter 2900: loss 1.3701, time 6.86ms, mfu 12.14%\n",
            "iter 2910: loss 1.3351, time 6.99ms, mfu 12.20%\n",
            "iter 2920: loss 1.3660, time 7.20ms, mfu 12.21%\n",
            "iter 2930: loss 1.3573, time 7.87ms, mfu 12.11%\n",
            "iter 2940: loss 1.3440, time 7.01ms, mfu 12.16%\n",
            "iter 2950: loss 1.3740, time 6.98ms, mfu 12.21%\n",
            "iter 2960: loss 1.3540, time 6.86ms, mfu 12.28%\n",
            "iter 2970: loss 1.3723, time 6.78ms, mfu 12.36%\n",
            "iter 2980: loss 1.3836, time 9.45ms, mfu 12.06%\n",
            "iter 2990: loss 1.3585, time 7.03ms, mfu 12.11%\n",
            "step 3000: train loss 1.2348, val loss 1.5150\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.3373, time 3961.21ms, mfu 10.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --max_iters=3000 --n_layer=3 --n_head=5 --n_embd=320 --dtype=float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdxS5Qkg9FQe",
        "outputId": "da1175ff-bb22-4007-8887-c37fc06b542a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: n_layer = 3\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 320\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.71M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 14, with 3,789,120 parameters\n",
            "num non-decayed parameter tensors: 7, with 2,240 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0427 06:35:08.831000 4092 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.2743, val loss 4.2667\n",
            "iter 0: loss 4.2669, time 19129.10ms, mfu -100.00%\n",
            "iter 10: loss 3.3462, time 36.16ms, mfu 3.66%\n",
            "iter 20: loss 2.9170, time 36.62ms, mfu 3.66%\n",
            "iter 30: loss 2.7139, time 37.70ms, mfu 3.64%\n",
            "iter 40: loss 2.5925, time 37.70ms, mfu 3.63%\n",
            "iter 50: loss 2.5496, time 35.17ms, mfu 3.64%\n",
            "iter 60: loss 2.5111, time 39.36ms, mfu 3.61%\n",
            "iter 70: loss 2.5055, time 36.36ms, mfu 3.62%\n",
            "iter 80: loss 2.4756, time 35.51ms, mfu 3.63%\n",
            "iter 90: loss 2.4899, time 39.23ms, mfu 3.60%\n",
            "iter 100: loss 2.4882, time 38.97ms, mfu 3.58%\n",
            "iter 110: loss 2.4681, time 31.55ms, mfu 3.64%\n",
            "iter 120: loss 2.4677, time 38.35ms, mfu 3.62%\n",
            "iter 130: loss 2.4509, time 37.10ms, mfu 3.62%\n",
            "iter 140: loss 2.4559, time 36.21ms, mfu 3.62%\n",
            "iter 150: loss 2.4171, time 34.57ms, mfu 3.64%\n",
            "iter 160: loss 2.4106, time 36.03ms, mfu 3.65%\n",
            "iter 170: loss 2.3982, time 36.67ms, mfu 3.64%\n",
            "iter 180: loss 2.3739, time 37.33ms, mfu 3.63%\n",
            "iter 190: loss 2.3749, time 40.66ms, mfu 3.59%\n",
            "iter 200: loss 2.3578, time 37.05ms, mfu 3.59%\n",
            "iter 210: loss 2.3101, time 36.28ms, mfu 3.60%\n",
            "iter 220: loss 2.2957, time 37.06ms, mfu 3.60%\n",
            "iter 230: loss 2.2560, time 37.73ms, mfu 3.59%\n",
            "iter 240: loss 2.2292, time 36.89ms, mfu 3.59%\n",
            "step 250: train loss 2.0787, val loss 2.1468\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.1531, time 4894.44ms, mfu 3.23%\n",
            "iter 260: loss 2.1232, time 37.29ms, mfu 3.26%\n",
            "iter 270: loss 2.0880, time 35.40ms, mfu 3.31%\n",
            "iter 280: loss 2.0841, time 53.11ms, mfu 3.23%\n",
            "iter 290: loss 2.0597, time 40.43ms, mfu 3.23%\n",
            "iter 300: loss 2.0338, time 35.75ms, mfu 3.28%\n",
            "iter 310: loss 1.9919, time 38.02ms, mfu 3.30%\n",
            "iter 320: loss 1.9589, time 35.54ms, mfu 3.34%\n",
            "iter 330: loss 1.9482, time 42.17ms, mfu 3.32%\n",
            "iter 340: loss 1.9269, time 38.48ms, mfu 3.33%\n",
            "iter 350: loss 1.9004, time 36.78ms, mfu 3.36%\n",
            "iter 360: loss 1.8637, time 36.85ms, mfu 3.38%\n",
            "iter 370: loss 1.8438, time 35.64ms, mfu 3.42%\n",
            "iter 380: loss 1.8795, time 40.63ms, mfu 3.40%\n",
            "iter 390: loss 1.8548, time 38.38ms, mfu 3.41%\n",
            "iter 400: loss 1.8037, time 35.17ms, mfu 3.44%\n",
            "iter 410: loss 1.7925, time 39.16ms, mfu 3.44%\n",
            "iter 420: loss 1.8078, time 36.77ms, mfu 3.45%\n",
            "iter 430: loss 1.8353, time 38.78ms, mfu 3.45%\n",
            "iter 440: loss 1.7751, time 38.94ms, mfu 3.44%\n",
            "iter 450: loss 1.7405, time 37.52ms, mfu 3.45%\n",
            "iter 460: loss 1.7427, time 37.86ms, mfu 3.46%\n",
            "iter 470: loss 1.7418, time 39.07ms, mfu 3.45%\n",
            "iter 480: loss 1.7398, time 36.83ms, mfu 3.46%\n",
            "iter 490: loss 1.7458, time 38.53ms, mfu 3.46%\n",
            "step 500: train loss 1.6034, val loss 1.7775\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6973, time 5036.70ms, mfu 3.12%\n",
            "iter 510: loss 1.6890, time 39.22ms, mfu 3.14%\n",
            "iter 520: loss 1.6998, time 38.06ms, mfu 3.18%\n",
            "iter 530: loss 1.6944, time 40.86ms, mfu 3.18%\n",
            "iter 540: loss 1.6937, time 34.52ms, mfu 3.25%\n",
            "iter 550: loss 1.6842, time 37.98ms, mfu 3.27%\n",
            "iter 560: loss 1.6977, time 38.98ms, mfu 3.28%\n",
            "iter 570: loss 1.6607, time 37.53ms, mfu 3.31%\n",
            "iter 580: loss 1.6261, time 42.26ms, mfu 3.29%\n",
            "iter 590: loss 1.6236, time 40.62ms, mfu 3.29%\n",
            "iter 600: loss 1.5920, time 41.82ms, mfu 3.28%\n",
            "iter 610: loss 1.6422, time 37.45ms, mfu 3.30%\n",
            "iter 620: loss 1.6055, time 36.04ms, mfu 3.34%\n",
            "iter 630: loss 1.6315, time 42.53ms, mfu 3.32%\n",
            "iter 640: loss 1.6130, time 39.75ms, mfu 3.32%\n",
            "iter 650: loss 1.5992, time 35.83ms, mfu 3.35%\n",
            "iter 660: loss 1.5982, time 39.24ms, mfu 3.36%\n",
            "iter 670: loss 1.5965, time 38.98ms, mfu 3.36%\n",
            "iter 680: loss 1.6334, time 41.26ms, mfu 3.35%\n",
            "iter 690: loss 1.5813, time 39.99ms, mfu 3.34%\n",
            "iter 700: loss 1.6179, time 39.36ms, mfu 3.34%\n",
            "iter 710: loss 1.5513, time 39.26ms, mfu 3.35%\n",
            "iter 720: loss 1.6096, time 40.55ms, mfu 3.34%\n",
            "iter 730: loss 1.6090, time 41.59ms, mfu 3.32%\n",
            "iter 740: loss 1.5557, time 39.91ms, mfu 3.32%\n",
            "step 750: train loss 1.4595, val loss 1.6616\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.5859, time 5226.86ms, mfu 2.99%\n",
            "iter 760: loss 1.5366, time 39.48ms, mfu 3.03%\n",
            "iter 770: loss 1.5624, time 38.55ms, mfu 3.07%\n",
            "iter 780: loss 1.5850, time 40.25ms, mfu 3.09%\n",
            "iter 790: loss 1.5539, time 40.35ms, mfu 3.11%\n",
            "iter 800: loss 1.5599, time 41.23ms, mfu 3.12%\n",
            "iter 810: loss 1.5523, time 39.27ms, mfu 3.14%\n",
            "iter 820: loss 1.5718, time 38.94ms, mfu 3.17%\n",
            "iter 830: loss 1.5396, time 39.09ms, mfu 3.19%\n",
            "iter 840: loss 1.5565, time 40.07ms, mfu 3.20%\n",
            "iter 850: loss 1.5415, time 43.51ms, mfu 3.19%\n",
            "iter 860: loss 1.5461, time 38.43ms, mfu 3.21%\n",
            "iter 870: loss 1.5128, time 40.17ms, mfu 3.22%\n",
            "iter 880: loss 1.5028, time 41.21ms, mfu 3.22%\n",
            "iter 890: loss 1.5317, time 40.10ms, mfu 3.23%\n",
            "iter 900: loss 1.5010, time 42.47ms, mfu 3.22%\n",
            "iter 910: loss 1.5004, time 39.88ms, mfu 3.23%\n",
            "iter 920: loss 1.5058, time 41.67ms, mfu 3.22%\n",
            "iter 930: loss 1.5097, time 40.61ms, mfu 3.23%\n",
            "iter 940: loss 1.4879, time 41.10ms, mfu 3.23%\n",
            "iter 950: loss 1.4907, time 40.33ms, mfu 3.23%\n",
            "iter 960: loss 1.4976, time 38.59ms, mfu 3.25%\n",
            "iter 970: loss 1.4776, time 41.01ms, mfu 3.25%\n",
            "iter 980: loss 1.4996, time 39.21ms, mfu 3.26%\n",
            "iter 990: loss 1.4891, time 35.04ms, mfu 3.31%\n",
            "step 1000: train loss 1.3808, val loss 1.6088\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.4573, time 5180.32ms, mfu 2.98%\n",
            "iter 1010: loss 1.4902, time 39.76ms, mfu 3.02%\n",
            "iter 1020: loss 1.4733, time 39.53ms, mfu 3.05%\n",
            "iter 1030: loss 1.4599, time 38.62ms, mfu 3.09%\n",
            "iter 1040: loss 1.4736, time 39.55ms, mfu 3.11%\n",
            "iter 1050: loss 1.4917, time 42.20ms, mfu 3.12%\n",
            "iter 1060: loss 1.4903, time 39.49ms, mfu 3.14%\n",
            "iter 1070: loss 1.4893, time 39.03ms, mfu 3.17%\n",
            "iter 1080: loss 1.5050, time 40.33ms, mfu 3.18%\n",
            "iter 1090: loss 1.4630, time 39.27ms, mfu 3.20%\n",
            "iter 1100: loss 1.4662, time 41.64ms, mfu 3.19%\n",
            "iter 1110: loss 1.4668, time 39.75ms, mfu 3.21%\n",
            "iter 1120: loss 1.4494, time 41.10ms, mfu 3.21%\n",
            "iter 1130: loss 1.4754, time 40.70ms, mfu 3.21%\n",
            "iter 1140: loss 1.4274, time 38.72ms, mfu 3.23%\n",
            "iter 1150: loss 1.4441, time 43.09ms, mfu 3.22%\n",
            "iter 1160: loss 1.4643, time 39.14ms, mfu 3.23%\n",
            "iter 1170: loss 1.4362, time 40.46ms, mfu 3.24%\n",
            "iter 1180: loss 1.4937, time 40.04ms, mfu 3.24%\n",
            "iter 1190: loss 1.4443, time 40.27ms, mfu 3.25%\n",
            "iter 1200: loss 1.4568, time 42.53ms, mfu 3.24%\n",
            "iter 1210: loss 1.4359, time 39.02ms, mfu 3.25%\n",
            "iter 1220: loss 1.4645, time 32.91ms, mfu 3.33%\n",
            "iter 1230: loss 1.4547, time 41.24ms, mfu 3.32%\n",
            "iter 1240: loss 1.4091, time 40.10ms, mfu 3.31%\n",
            "step 1250: train loss 1.3228, val loss 1.5562\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.4311, time 5056.75ms, mfu 2.99%\n",
            "iter 1260: loss 1.4364, time 39.18ms, mfu 3.03%\n",
            "iter 1270: loss 1.4044, time 35.92ms, mfu 3.09%\n",
            "iter 1280: loss 1.4211, time 37.91ms, mfu 3.13%\n",
            "iter 1290: loss 1.4032, time 37.04ms, mfu 3.18%\n",
            "iter 1300: loss 1.4129, time 41.22ms, mfu 3.18%\n",
            "iter 1310: loss 1.4281, time 39.00ms, mfu 3.20%\n",
            "iter 1320: loss 1.4543, time 38.59ms, mfu 3.22%\n",
            "iter 1330: loss 1.4018, time 39.69ms, mfu 3.23%\n",
            "iter 1340: loss 1.4068, time 40.64ms, mfu 3.24%\n",
            "iter 1350: loss 1.4263, time 42.39ms, mfu 3.23%\n",
            "iter 1360: loss 1.4434, time 38.25ms, mfu 3.25%\n",
            "iter 1370: loss 1.4124, time 38.82ms, mfu 3.26%\n",
            "iter 1380: loss 1.4120, time 41.34ms, mfu 3.26%\n",
            "iter 1390: loss 1.3965, time 39.13ms, mfu 3.27%\n",
            "iter 1400: loss 1.4618, time 41.35ms, mfu 3.26%\n",
            "iter 1410: loss 1.4038, time 38.99ms, mfu 3.28%\n",
            "iter 1420: loss 1.4399, time 40.61ms, mfu 3.28%\n",
            "iter 1430: loss 1.3949, time 40.87ms, mfu 3.27%\n",
            "iter 1440: loss 1.4088, time 40.27ms, mfu 3.27%\n",
            "iter 1450: loss 1.4144, time 39.45ms, mfu 3.28%\n",
            "iter 1460: loss 1.4097, time 38.95ms, mfu 3.29%\n",
            "iter 1470: loss 1.3713, time 41.55ms, mfu 3.28%\n",
            "iter 1480: loss 1.4149, time 38.98ms, mfu 3.29%\n",
            "iter 1490: loss 1.4037, time 39.97ms, mfu 3.30%\n",
            "step 1500: train loss 1.2828, val loss 1.5240\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.3873, time 5045.37ms, mfu 2.97%\n",
            "iter 1510: loss 1.3584, time 39.61ms, mfu 3.01%\n",
            "iter 1520: loss 1.3821, time 37.98ms, mfu 3.05%\n",
            "iter 1530: loss 1.3830, time 42.54ms, mfu 3.06%\n",
            "iter 1540: loss 1.3784, time 38.86ms, mfu 3.09%\n",
            "iter 1550: loss 1.3496, time 39.16ms, mfu 3.12%\n",
            "iter 1560: loss 1.3938, time 31.49ms, mfu 3.23%\n",
            "iter 1570: loss 1.3601, time 38.24ms, mfu 3.25%\n",
            "iter 1580: loss 1.3826, time 42.65ms, mfu 3.24%\n",
            "iter 1590: loss 1.3612, time 40.09ms, mfu 3.25%\n",
            "iter 1600: loss 1.4152, time 42.86ms, mfu 3.23%\n",
            "iter 1610: loss 1.3829, time 39.42ms, mfu 3.24%\n",
            "iter 1620: loss 1.3715, time 39.36ms, mfu 3.25%\n",
            "iter 1630: loss 1.3636, time 41.46ms, mfu 3.25%\n",
            "iter 1640: loss 1.3870, time 39.30ms, mfu 3.26%\n",
            "iter 1650: loss 1.3684, time 39.59ms, mfu 3.27%\n",
            "iter 1660: loss 1.4019, time 39.45ms, mfu 3.28%\n",
            "iter 1670: loss 1.3862, time 41.18ms, mfu 3.27%\n",
            "iter 1680: loss 1.3400, time 40.05ms, mfu 3.27%\n",
            "iter 1690: loss 1.3704, time 40.22ms, mfu 3.28%\n",
            "iter 1700: loss 1.3972, time 41.81ms, mfu 3.26%\n",
            "iter 1710: loss 1.3848, time 40.49ms, mfu 3.27%\n",
            "iter 1720: loss 1.3581, time 39.47ms, mfu 3.27%\n",
            "iter 1730: loss 1.3694, time 40.31ms, mfu 3.28%\n",
            "iter 1740: loss 1.3549, time 41.34ms, mfu 3.27%\n",
            "step 1750: train loss 1.2486, val loss 1.5181\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.3364, time 5213.98ms, mfu 2.94%\n",
            "iter 1760: loss 1.3611, time 33.42ms, mfu 3.05%\n",
            "iter 1770: loss 1.3866, time 38.92ms, mfu 3.08%\n",
            "iter 1780: loss 1.3989, time 39.60ms, mfu 3.11%\n",
            "iter 1790: loss 1.3344, time 39.80ms, mfu 3.13%\n",
            "iter 1800: loss 1.3444, time 43.84ms, mfu 3.12%\n",
            "iter 1810: loss 1.3591, time 38.91ms, mfu 3.15%\n",
            "iter 1820: loss 1.3505, time 40.90ms, mfu 3.16%\n",
            "iter 1830: loss 1.3569, time 41.69ms, mfu 3.16%\n",
            "iter 1840: loss 1.3387, time 40.33ms, mfu 3.17%\n",
            "iter 1850: loss 1.3136, time 43.32ms, mfu 3.16%\n",
            "iter 1860: loss 1.3688, time 40.54ms, mfu 3.17%\n",
            "iter 1870: loss 1.3777, time 41.02ms, mfu 3.17%\n",
            "iter 1880: loss 1.3312, time 39.82ms, mfu 3.19%\n",
            "iter 1890: loss 1.3476, time 40.05ms, mfu 3.20%\n",
            "iter 1900: loss 1.3428, time 43.16ms, mfu 3.19%\n",
            "iter 1910: loss 1.3655, time 41.89ms, mfu 3.18%\n",
            "iter 1920: loss 1.3454, time 42.28ms, mfu 3.18%\n",
            "iter 1930: loss 1.3134, time 40.73ms, mfu 3.19%\n",
            "iter 1940: loss 1.3572, time 41.61ms, mfu 3.19%\n",
            "iter 1950: loss 1.3571, time 43.70ms, mfu 3.17%\n",
            "iter 1960: loss 1.3472, time 41.22ms, mfu 3.17%\n",
            "iter 1970: loss 1.3192, time 41.48ms, mfu 3.18%\n",
            "iter 1980: loss 1.3570, time 40.97ms, mfu 3.18%\n",
            "iter 1990: loss 1.3464, time 42.63ms, mfu 3.17%\n",
            "step 2000: train loss 1.2235, val loss 1.4948\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.3723, time 5348.06ms, mfu 2.86%\n",
            "iter 2010: loss 1.3049, time 40.71ms, mfu 2.90%\n",
            "iter 2020: loss 1.3291, time 40.91ms, mfu 2.93%\n",
            "iter 2030: loss 1.3452, time 40.73ms, mfu 2.96%\n",
            "iter 2040: loss 1.2931, time 41.22ms, mfu 2.99%\n",
            "iter 2050: loss 1.3198, time 52.34ms, mfu 2.94%\n",
            "iter 2060: loss 1.3163, time 40.30ms, mfu 2.98%\n",
            "iter 2070: loss 1.3115, time 44.34ms, mfu 2.98%\n",
            "iter 2080: loss 1.3636, time 43.87ms, mfu 2.98%\n",
            "iter 2090: loss 1.3370, time 40.62ms, mfu 3.01%\n",
            "iter 2100: loss 1.3367, time 44.40ms, mfu 3.01%\n",
            "iter 2110: loss 1.2988, time 44.60ms, mfu 3.00%\n",
            "iter 2120: loss 1.3296, time 46.13ms, mfu 2.99%\n",
            "iter 2130: loss 1.3510, time 40.66ms, mfu 3.02%\n",
            "iter 2140: loss 1.3514, time 41.93ms, mfu 3.03%\n",
            "iter 2150: loss 1.3377, time 41.85ms, mfu 3.04%\n",
            "iter 2160: loss 1.3321, time 45.19ms, mfu 3.03%\n",
            "iter 2170: loss 1.3227, time 45.34ms, mfu 3.02%\n",
            "iter 2180: loss 1.3397, time 42.57ms, mfu 3.03%\n",
            "iter 2190: loss 1.3169, time 44.42ms, mfu 3.02%\n",
            "iter 2200: loss 1.3061, time 41.56ms, mfu 3.04%\n",
            "iter 2210: loss 1.2927, time 44.58ms, mfu 3.03%\n",
            "iter 2220: loss 1.3274, time 41.34ms, mfu 3.05%\n",
            "iter 2230: loss 1.3049, time 43.27ms, mfu 3.05%\n",
            "iter 2240: loss 1.3148, time 43.87ms, mfu 3.05%\n",
            "step 2250: train loss 1.1987, val loss 1.4814\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.3215, time 5271.71ms, mfu 2.75%\n",
            "iter 2260: loss 1.2730, time 41.52ms, mfu 2.79%\n",
            "iter 2270: loss 1.3010, time 40.97ms, mfu 2.83%\n",
            "iter 2280: loss 1.3083, time 40.41ms, mfu 2.88%\n",
            "iter 2290: loss 1.3385, time 40.85ms, mfu 2.91%\n",
            "iter 2300: loss 1.3067, time 53.25ms, mfu 2.87%\n",
            "iter 2310: loss 1.2799, time 40.48ms, mfu 2.91%\n",
            "iter 2320: loss 1.3099, time 42.46ms, mfu 2.93%\n",
            "iter 2330: loss 1.3014, time 41.22ms, mfu 2.96%\n",
            "iter 2340: loss 1.3179, time 40.31ms, mfu 2.99%\n",
            "iter 2350: loss 1.3295, time 45.80ms, mfu 2.98%\n",
            "iter 2360: loss 1.2895, time 41.45ms, mfu 3.00%\n",
            "iter 2370: loss 1.3096, time 44.04ms, mfu 3.00%\n",
            "iter 2380: loss 1.2985, time 41.38ms, mfu 3.02%\n",
            "iter 2390: loss 1.3011, time 41.52ms, mfu 3.04%\n",
            "iter 2400: loss 1.3112, time 43.39ms, mfu 3.04%\n",
            "iter 2410: loss 1.3081, time 39.38ms, mfu 3.07%\n",
            "iter 2420: loss 1.2949, time 45.56ms, mfu 3.06%\n",
            "iter 2430: loss 1.3074, time 44.19ms, mfu 3.05%\n",
            "iter 2440: loss 1.2838, time 41.94ms, mfu 3.06%\n",
            "iter 2450: loss 1.2896, time 44.17ms, mfu 3.05%\n",
            "iter 2460: loss 1.3201, time 40.13ms, mfu 3.08%\n",
            "iter 2470: loss 1.2883, time 42.82ms, mfu 3.08%\n",
            "iter 2480: loss 1.3043, time 40.47ms, mfu 3.10%\n",
            "iter 2490: loss 1.2974, time 40.13ms, mfu 3.12%\n",
            "step 2500: train loss 1.1774, val loss 1.4766\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.2547, time 5203.26ms, mfu 2.81%\n",
            "iter 2510: loss 1.3179, time 39.98ms, mfu 2.86%\n",
            "iter 2520: loss 1.2958, time 40.43ms, mfu 2.90%\n",
            "iter 2530: loss 1.2925, time 42.11ms, mfu 2.93%\n",
            "iter 2540: loss 1.3003, time 40.68ms, mfu 2.96%\n",
            "iter 2550: loss 1.2887, time 47.27ms, mfu 2.94%\n",
            "iter 2560: loss 1.2714, time 39.62ms, mfu 2.98%\n",
            "iter 2570: loss 1.2724, time 40.32ms, mfu 3.01%\n",
            "iter 2580: loss 1.3001, time 41.88ms, mfu 3.03%\n",
            "iter 2590: loss 1.2756, time 39.38ms, mfu 3.06%\n",
            "iter 2600: loss 1.2579, time 41.91ms, mfu 3.07%\n",
            "iter 2610: loss 1.2548, time 39.90ms, mfu 3.09%\n",
            "iter 2620: loss 1.2477, time 41.75ms, mfu 3.10%\n",
            "iter 2630: loss 1.2680, time 40.15ms, mfu 3.12%\n",
            "iter 2640: loss 1.2969, time 39.65ms, mfu 3.14%\n",
            "iter 2650: loss 1.2964, time 43.89ms, mfu 3.13%\n",
            "iter 2660: loss 1.2938, time 40.17ms, mfu 3.15%\n",
            "iter 2670: loss 1.2747, time 39.50ms, mfu 3.17%\n",
            "iter 2680: loss 1.2854, time 40.53ms, mfu 3.18%\n",
            "iter 2690: loss 1.2733, time 39.96ms, mfu 3.19%\n",
            "iter 2700: loss 1.3294, time 41.65ms, mfu 3.19%\n",
            "iter 2710: loss 1.2594, time 39.29ms, mfu 3.21%\n",
            "iter 2720: loss 1.2789, time 42.78ms, mfu 3.20%\n",
            "iter 2730: loss 1.2951, time 41.66ms, mfu 3.19%\n",
            "iter 2740: loss 1.2913, time 41.69ms, mfu 3.19%\n",
            "step 2750: train loss 1.1565, val loss 1.4765\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.2952, time 5280.35ms, mfu 2.88%\n",
            "iter 2760: loss 1.3036, time 40.03ms, mfu 2.92%\n",
            "iter 2770: loss 1.2633, time 41.57ms, mfu 2.95%\n",
            "iter 2780: loss 1.2236, time 41.24ms, mfu 2.97%\n",
            "iter 2790: loss 1.2490, time 39.17ms, mfu 3.01%\n",
            "iter 2800: loss 1.2530, time 48.05ms, mfu 2.99%\n",
            "iter 2810: loss 1.2903, time 40.85ms, mfu 3.01%\n",
            "iter 2820: loss 1.2613, time 40.38ms, mfu 3.04%\n",
            "iter 2830: loss 1.2659, time 40.86ms, mfu 3.06%\n",
            "iter 2840: loss 1.2748, time 40.30ms, mfu 3.08%\n",
            "iter 2850: loss 1.2661, time 43.13ms, mfu 3.08%\n",
            "iter 2860: loss 1.2764, time 40.42ms, mfu 3.10%\n",
            "iter 2870: loss 1.2554, time 42.72ms, mfu 3.10%\n",
            "iter 2880: loss 1.2424, time 41.30ms, mfu 3.11%\n",
            "iter 2890: loss 1.2726, time 40.86ms, mfu 3.12%\n",
            "iter 2900: loss 1.2802, time 39.84ms, mfu 3.14%\n",
            "iter 2910: loss 1.2465, time 40.23ms, mfu 3.16%\n",
            "iter 2920: loss 1.2630, time 41.79ms, mfu 3.16%\n",
            "iter 2930: loss 1.2902, time 38.25ms, mfu 3.19%\n",
            "iter 2940: loss 1.3012, time 41.31ms, mfu 3.19%\n",
            "iter 2950: loss 1.2579, time 40.10ms, mfu 3.20%\n",
            "iter 2960: loss 1.2756, time 41.45ms, mfu 3.20%\n",
            "iter 2970: loss 1.3040, time 40.21ms, mfu 3.21%\n",
            "iter 2980: loss 1.2553, time 40.59ms, mfu 3.21%\n",
            "iter 2990: loss 1.2883, time 39.88ms, mfu 3.23%\n",
            "step 3000: train loss 1.1369, val loss 1.4710\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.2441, time 5256.01ms, mfu 2.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --max_iters=3000 --n_layer=5 --n_head=5 --n_embd=320 --dtype=float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHGHSW0t9-2c",
        "outputId": "122284e2-a699-47e0-dcf2-17e32d210eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: n_layer = 5\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 320\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 6.17M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 22, with 6,246,720 parameters\n",
            "num non-decayed parameter tensors: 11, with 3,520 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0427 06:38:46.201000 5070 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.2529, val loss 4.2533\n",
            "iter 0: loss 4.2519, time 26369.64ms, mfu -100.00%\n",
            "iter 10: loss 3.2850, time 63.96ms, mfu 3.44%\n",
            "iter 20: loss 2.8582, time 71.68ms, mfu 3.41%\n",
            "iter 30: loss 2.6537, time 63.84ms, mfu 3.41%\n",
            "iter 40: loss 2.5827, time 65.76ms, mfu 3.40%\n",
            "iter 50: loss 2.5427, time 73.28ms, mfu 3.36%\n",
            "iter 60: loss 2.5085, time 65.74ms, mfu 3.36%\n",
            "iter 70: loss 2.5075, time 66.46ms, mfu 3.36%\n",
            "iter 80: loss 2.4852, time 71.36ms, mfu 3.33%\n",
            "iter 90: loss 2.4828, time 68.07ms, mfu 3.32%\n",
            "iter 100: loss 2.4695, time 67.74ms, mfu 3.31%\n",
            "iter 110: loss 2.4706, time 72.76ms, mfu 3.28%\n",
            "iter 120: loss 2.4649, time 69.23ms, mfu 3.27%\n",
            "iter 130: loss 2.4295, time 67.96ms, mfu 3.27%\n",
            "iter 140: loss 2.3932, time 70.57ms, mfu 3.26%\n",
            "iter 150: loss 2.4002, time 69.12ms, mfu 3.25%\n",
            "iter 160: loss 2.4050, time 68.31ms, mfu 3.25%\n",
            "iter 170: loss 2.4188, time 73.53ms, mfu 3.22%\n",
            "iter 180: loss 2.3358, time 72.61ms, mfu 3.20%\n",
            "iter 190: loss 2.3479, time 73.52ms, mfu 3.18%\n",
            "iter 200: loss 2.3442, time 72.38ms, mfu 3.17%\n",
            "iter 210: loss 2.2960, time 71.79ms, mfu 3.16%\n",
            "iter 220: loss 2.2691, time 73.02ms, mfu 3.14%\n",
            "iter 230: loss 2.2402, time 73.62ms, mfu 3.13%\n",
            "iter 240: loss 2.2081, time 71.18ms, mfu 3.12%\n",
            "step 250: train loss 2.0941, val loss 2.1609\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.1854, time 8639.51ms, mfu 2.81%\n",
            "iter 260: loss 2.1610, time 71.83ms, mfu 2.84%\n",
            "iter 270: loss 2.1394, time 70.25ms, mfu 2.87%\n",
            "iter 280: loss 2.0882, time 78.79ms, mfu 2.86%\n",
            "iter 290: loss 2.0450, time 71.41ms, mfu 2.88%\n",
            "iter 300: loss 2.0138, time 72.07ms, mfu 2.90%\n",
            "iter 310: loss 2.0030, time 70.74ms, mfu 2.92%\n",
            "iter 320: loss 1.9718, time 77.75ms, mfu 2.91%\n",
            "iter 330: loss 1.9392, time 70.96ms, mfu 2.93%\n",
            "iter 340: loss 1.9248, time 69.62ms, mfu 2.95%\n",
            "iter 350: loss 1.9272, time 73.85ms, mfu 2.96%\n",
            "iter 360: loss 1.9057, time 71.38ms, mfu 2.97%\n",
            "iter 370: loss 1.8775, time 68.99ms, mfu 2.99%\n",
            "iter 380: loss 1.8531, time 69.34ms, mfu 3.01%\n",
            "iter 390: loss 1.8238, time 70.89ms, mfu 3.02%\n",
            "iter 400: loss 1.8307, time 67.88ms, mfu 3.04%\n",
            "iter 410: loss 1.8235, time 68.21ms, mfu 3.06%\n",
            "iter 420: loss 1.7877, time 69.71ms, mfu 3.07%\n",
            "iter 430: loss 1.7633, time 69.82ms, mfu 3.08%\n",
            "iter 440: loss 1.7845, time 69.56ms, mfu 3.09%\n",
            "iter 450: loss 1.7431, time 69.09ms, mfu 3.10%\n",
            "iter 460: loss 1.7192, time 71.68ms, mfu 3.09%\n",
            "iter 470: loss 1.6991, time 70.09ms, mfu 3.10%\n",
            "iter 480: loss 1.7358, time 68.67ms, mfu 3.11%\n",
            "iter 490: loss 1.7414, time 71.07ms, mfu 3.11%\n",
            "step 500: train loss 1.5991, val loss 1.7761\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.7399, time 8543.78ms, mfu 2.80%\n",
            "iter 510: loss 1.6775, time 65.78ms, mfu 2.86%\n",
            "iter 520: loss 1.6988, time 64.51ms, mfu 2.91%\n",
            "iter 530: loss 1.6799, time 66.61ms, mfu 2.95%\n",
            "iter 540: loss 1.6405, time 65.28ms, mfu 2.99%\n",
            "iter 550: loss 1.6550, time 65.54ms, mfu 3.03%\n",
            "iter 560: loss 1.6154, time 71.84ms, mfu 3.03%\n",
            "iter 570: loss 1.6005, time 62.85ms, mfu 3.08%\n",
            "iter 580: loss 1.6448, time 67.67ms, mfu 3.10%\n",
            "iter 590: loss 1.5850, time 69.66ms, mfu 3.10%\n",
            "iter 600: loss 1.5863, time 64.36ms, mfu 3.14%\n",
            "iter 610: loss 1.5920, time 68.46ms, mfu 3.14%\n",
            "iter 620: loss 1.5919, time 68.30ms, mfu 3.15%\n",
            "iter 630: loss 1.5979, time 67.16ms, mfu 3.16%\n",
            "iter 640: loss 1.5679, time 68.58ms, mfu 3.17%\n",
            "iter 650: loss 1.5462, time 69.03ms, mfu 3.17%\n",
            "iter 660: loss 1.5642, time 65.84ms, mfu 3.19%\n",
            "iter 670: loss 1.5533, time 68.25ms, mfu 3.19%\n",
            "iter 680: loss 1.5423, time 68.88ms, mfu 3.19%\n",
            "iter 690: loss 1.5432, time 69.03ms, mfu 3.19%\n",
            "iter 700: loss 1.4911, time 67.20ms, mfu 3.20%\n",
            "iter 710: loss 1.5354, time 67.97ms, mfu 3.20%\n",
            "iter 720: loss 1.5142, time 67.95ms, mfu 3.21%\n",
            "iter 730: loss 1.5097, time 66.52ms, mfu 3.22%\n",
            "iter 740: loss 1.5005, time 67.62ms, mfu 3.22%\n",
            "step 750: train loss 1.4190, val loss 1.6229\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.5140, time 8473.29ms, mfu 2.90%\n",
            "iter 760: loss 1.5146, time 63.63ms, mfu 2.96%\n",
            "iter 770: loss 1.4865, time 67.49ms, mfu 2.99%\n",
            "iter 780: loss 1.4936, time 64.32ms, mfu 3.03%\n",
            "iter 790: loss 1.5130, time 62.98ms, mfu 3.08%\n",
            "iter 800: loss 1.5020, time 69.02ms, mfu 3.09%\n",
            "iter 810: loss 1.4506, time 67.21ms, mfu 3.11%\n",
            "iter 820: loss 1.4372, time 63.48ms, mfu 3.14%\n",
            "iter 830: loss 1.4796, time 71.86ms, mfu 3.14%\n",
            "iter 840: loss 1.4417, time 71.28ms, mfu 3.13%\n",
            "iter 850: loss 1.4559, time 65.75ms, mfu 3.15%\n",
            "iter 860: loss 1.4355, time 68.77ms, mfu 3.16%\n",
            "iter 870: loss 1.4321, time 65.88ms, mfu 3.18%\n",
            "iter 880: loss 1.4435, time 67.52ms, mfu 3.18%\n",
            "iter 890: loss 1.4529, time 69.47ms, mfu 3.18%\n",
            "iter 900: loss 1.4268, time 69.39ms, mfu 3.18%\n",
            "iter 910: loss 1.4511, time 69.69ms, mfu 3.18%\n",
            "iter 920: loss 1.4783, time 69.56ms, mfu 3.18%\n",
            "iter 930: loss 1.4293, time 71.36ms, mfu 3.17%\n",
            "iter 940: loss 1.4096, time 70.12ms, mfu 3.17%\n",
            "iter 950: loss 1.3941, time 68.86ms, mfu 3.17%\n",
            "iter 960: loss 1.3958, time 68.60ms, mfu 3.17%\n",
            "iter 970: loss 1.4355, time 66.31ms, mfu 3.19%\n",
            "iter 980: loss 1.3738, time 66.81ms, mfu 3.20%\n",
            "iter 990: loss 1.4255, time 67.70ms, mfu 3.20%\n",
            "step 1000: train loss 1.3216, val loss 1.5555\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.4178, time 8533.70ms, mfu 2.89%\n",
            "iter 1010: loss 1.3862, time 65.89ms, mfu 2.93%\n",
            "iter 1020: loss 1.4190, time 67.95ms, mfu 2.96%\n",
            "iter 1030: loss 1.4024, time 82.34ms, mfu 2.93%\n",
            "iter 1040: loss 1.4060, time 67.78ms, mfu 2.97%\n",
            "iter 1050: loss 1.3895, time 70.18ms, mfu 2.98%\n",
            "iter 1060: loss 1.4223, time 75.45ms, mfu 2.98%\n",
            "iter 1070: loss 1.4067, time 71.38ms, mfu 2.99%\n",
            "iter 1080: loss 1.3684, time 68.31ms, mfu 3.01%\n",
            "iter 1090: loss 1.3848, time 70.30ms, mfu 3.02%\n",
            "iter 1100: loss 1.3540, time 70.63ms, mfu 3.03%\n",
            "iter 1110: loss 1.3663, time 69.02ms, mfu 3.05%\n",
            "iter 1120: loss 1.3656, time 69.63ms, mfu 3.06%\n",
            "iter 1130: loss 1.3646, time 66.97ms, mfu 3.08%\n",
            "iter 1140: loss 1.3347, time 71.82ms, mfu 3.08%\n",
            "iter 1150: loss 1.3337, time 68.59ms, mfu 3.09%\n",
            "iter 1160: loss 1.3673, time 71.37ms, mfu 3.09%\n",
            "iter 1170: loss 1.3792, time 68.75ms, mfu 3.10%\n",
            "iter 1180: loss 1.3754, time 71.89ms, mfu 3.10%\n",
            "iter 1190: loss 1.3673, time 73.75ms, mfu 3.09%\n",
            "iter 1200: loss 1.3836, time 71.27ms, mfu 3.09%\n",
            "iter 1210: loss 1.3136, time 69.42ms, mfu 3.10%\n",
            "iter 1220: loss 1.3158, time 69.78ms, mfu 3.10%\n",
            "iter 1230: loss 1.3555, time 70.03ms, mfu 3.11%\n",
            "iter 1240: loss 1.3327, time 71.56ms, mfu 3.10%\n",
            "step 1250: train loss 1.2616, val loss 1.5117\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.3535, time 8548.28ms, mfu 2.80%\n",
            "iter 1260: loss 1.3175, time 64.52ms, mfu 2.86%\n",
            "iter 1270: loss 1.3571, time 67.80ms, mfu 2.90%\n",
            "iter 1280: loss 1.3554, time 75.34ms, mfu 2.90%\n",
            "iter 1290: loss 1.3308, time 66.70ms, mfu 2.94%\n",
            "iter 1300: loss 1.3546, time 67.69ms, mfu 2.97%\n",
            "iter 1310: loss 1.3214, time 73.27ms, mfu 2.97%\n",
            "iter 1320: loss 1.3258, time 67.90ms, mfu 3.00%\n",
            "iter 1330: loss 1.3353, time 67.67ms, mfu 3.03%\n",
            "iter 1340: loss 1.3294, time 72.29ms, mfu 3.03%\n",
            "iter 1350: loss 1.3469, time 71.58ms, mfu 3.03%\n",
            "iter 1360: loss 1.3466, time 68.37ms, mfu 3.05%\n",
            "iter 1370: loss 1.3203, time 71.58ms, mfu 3.05%\n",
            "iter 1380: loss 1.3108, time 69.68ms, mfu 3.06%\n",
            "iter 1390: loss 1.2928, time 67.76ms, mfu 3.08%\n",
            "iter 1400: loss 1.2880, time 69.69ms, mfu 3.09%\n",
            "iter 1410: loss 1.3323, time 71.82ms, mfu 3.09%\n",
            "iter 1420: loss 1.3267, time 67.78ms, mfu 3.10%\n",
            "iter 1430: loss 1.3095, time 70.22ms, mfu 3.11%\n",
            "iter 1440: loss 1.3092, time 69.53ms, mfu 3.11%\n",
            "iter 1450: loss 1.3362, time 67.68ms, mfu 3.13%\n",
            "iter 1460: loss 1.2822, time 67.26ms, mfu 3.14%\n",
            "iter 1470: loss 1.3111, time 67.07ms, mfu 3.16%\n",
            "iter 1480: loss 1.3031, time 67.74ms, mfu 3.17%\n",
            "iter 1490: loss 1.3193, time 68.17ms, mfu 3.17%\n",
            "step 1500: train loss 1.2150, val loss 1.4823\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.3041, time 8534.09ms, mfu 2.86%\n",
            "iter 1510: loss 1.3058, time 65.79ms, mfu 2.91%\n",
            "iter 1520: loss 1.2986, time 65.28ms, mfu 2.95%\n",
            "iter 1530: loss 1.3029, time 72.65ms, mfu 2.96%\n",
            "iter 1540: loss 1.3390, time 65.10ms, mfu 3.00%\n",
            "iter 1550: loss 1.3311, time 64.46ms, mfu 3.04%\n",
            "iter 1560: loss 1.2662, time 73.60ms, mfu 3.04%\n",
            "iter 1570: loss 1.2953, time 64.90ms, mfu 3.07%\n",
            "iter 1580: loss 1.2741, time 69.68ms, mfu 3.08%\n",
            "iter 1590: loss 1.2881, time 71.96ms, mfu 3.08%\n",
            "iter 1600: loss 1.2741, time 65.55ms, mfu 3.11%\n",
            "iter 1610: loss 1.3284, time 66.70ms, mfu 3.13%\n",
            "iter 1620: loss 1.2974, time 70.41ms, mfu 3.13%\n",
            "iter 1630: loss 1.3007, time 65.92ms, mfu 3.15%\n",
            "iter 1640: loss 1.2641, time 63.97ms, mfu 3.18%\n",
            "iter 1650: loss 1.2847, time 67.87ms, mfu 3.18%\n",
            "iter 1660: loss 1.2961, time 69.07ms, mfu 3.18%\n",
            "iter 1670: loss 1.2549, time 66.56ms, mfu 3.20%\n",
            "iter 1680: loss 1.2696, time 69.21ms, mfu 3.20%\n",
            "iter 1690: loss 1.2557, time 69.43ms, mfu 3.19%\n",
            "iter 1700: loss 1.2614, time 70.11ms, mfu 3.19%\n",
            "iter 1710: loss 1.2674, time 70.85ms, mfu 3.18%\n",
            "iter 1720: loss 1.2730, time 68.68ms, mfu 3.18%\n",
            "iter 1730: loss 1.2668, time 65.52ms, mfu 3.20%\n",
            "iter 1740: loss 1.2578, time 68.04ms, mfu 3.20%\n",
            "step 1750: train loss 1.1736, val loss 1.4772\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.2827, time 8543.35ms, mfu 2.89%\n",
            "iter 1760: loss 1.2762, time 66.44ms, mfu 2.93%\n",
            "iter 1770: loss 1.2796, time 64.11ms, mfu 2.98%\n",
            "iter 1780: loss 1.2495, time 76.08ms, mfu 2.97%\n",
            "iter 1790: loss 1.2706, time 66.03ms, mfu 3.01%\n",
            "iter 1800: loss 1.2510, time 66.06ms, mfu 3.04%\n",
            "iter 1810: loss 1.2559, time 70.55ms, mfu 3.05%\n",
            "iter 1820: loss 1.2396, time 64.63ms, mfu 3.08%\n",
            "iter 1830: loss 1.2514, time 67.49ms, mfu 3.10%\n",
            "iter 1840: loss 1.2746, time 71.76ms, mfu 3.10%\n",
            "iter 1850: loss 1.2591, time 69.66ms, mfu 3.10%\n",
            "iter 1860: loss 1.2593, time 69.05ms, mfu 3.11%\n",
            "iter 1870: loss 1.2361, time 70.02ms, mfu 3.12%\n",
            "iter 1880: loss 1.2191, time 67.66ms, mfu 3.13%\n",
            "iter 1890: loss 1.2337, time 64.63ms, mfu 3.16%\n",
            "iter 1900: loss 1.2609, time 67.06ms, mfu 3.17%\n",
            "iter 1910: loss 1.2805, time 65.66ms, mfu 3.19%\n",
            "iter 1920: loss 1.2607, time 65.46ms, mfu 3.21%\n",
            "iter 1930: loss 1.2591, time 69.07ms, mfu 3.20%\n",
            "iter 1940: loss 1.2430, time 71.85ms, mfu 3.19%\n",
            "iter 1950: loss 1.2364, time 70.73ms, mfu 3.18%\n",
            "iter 1960: loss 1.2360, time 69.22ms, mfu 3.18%\n",
            "iter 1970: loss 1.2322, time 70.66ms, mfu 3.18%\n",
            "iter 1980: loss 1.2083, time 68.61ms, mfu 3.18%\n",
            "iter 1990: loss 1.2002, time 66.51ms, mfu 3.19%\n",
            "step 2000: train loss 1.1424, val loss 1.4687\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.2178, time 8529.20ms, mfu 2.88%\n",
            "iter 2010: loss 1.2424, time 66.99ms, mfu 2.92%\n",
            "iter 2020: loss 1.2388, time 66.90ms, mfu 2.95%\n",
            "iter 2030: loss 1.2335, time 75.94ms, mfu 2.95%\n",
            "iter 2040: loss 1.2213, time 66.59ms, mfu 2.98%\n",
            "iter 2050: loss 1.2093, time 67.67ms, mfu 3.01%\n",
            "iter 2060: loss 1.2416, time 71.96ms, mfu 3.02%\n",
            "iter 2070: loss 1.2189, time 68.79ms, mfu 3.03%\n",
            "iter 2080: loss 1.2077, time 70.22ms, mfu 3.04%\n",
            "iter 2090: loss 1.2104, time 72.58ms, mfu 3.04%\n",
            "iter 2100: loss 1.2127, time 69.79ms, mfu 3.05%\n",
            "iter 2110: loss 1.2155, time 68.08ms, mfu 3.07%\n",
            "iter 2120: loss 1.2209, time 72.15ms, mfu 3.07%\n",
            "iter 2130: loss 1.1942, time 68.72ms, mfu 3.08%\n",
            "iter 2140: loss 1.2138, time 66.93ms, mfu 3.10%\n",
            "iter 2150: loss 1.2254, time 69.98ms, mfu 3.11%\n",
            "iter 2160: loss 1.2083, time 71.69ms, mfu 3.10%\n",
            "iter 2170: loss 1.2260, time 67.36ms, mfu 3.12%\n",
            "iter 2180: loss 1.1954, time 71.42ms, mfu 3.12%\n",
            "iter 2190: loss 1.2386, time 70.57ms, mfu 3.12%\n",
            "iter 2200: loss 1.2161, time 69.05ms, mfu 3.12%\n",
            "iter 2210: loss 1.2305, time 66.39ms, mfu 3.14%\n",
            "iter 2220: loss 1.1813, time 72.54ms, mfu 3.13%\n",
            "iter 2230: loss 1.1978, time 68.07ms, mfu 3.14%\n",
            "iter 2240: loss 1.2201, time 71.07ms, mfu 3.14%\n",
            "step 2250: train loss 1.1118, val loss 1.4584\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.2252, time 8536.65ms, mfu 2.83%\n",
            "iter 2260: loss 1.2347, time 65.63ms, mfu 2.88%\n",
            "iter 2270: loss 1.1936, time 69.69ms, mfu 2.91%\n",
            "iter 2280: loss 1.1935, time 72.46ms, mfu 2.92%\n",
            "iter 2290: loss 1.1871, time 67.28ms, mfu 2.96%\n",
            "iter 2300: loss 1.2183, time 69.63ms, mfu 2.98%\n",
            "iter 2310: loss 1.2149, time 70.68ms, mfu 2.99%\n",
            "iter 2320: loss 1.1805, time 69.15ms, mfu 3.01%\n",
            "iter 2330: loss 1.2229, time 67.71ms, mfu 3.03%\n",
            "iter 2340: loss 1.1677, time 68.39ms, mfu 3.05%\n",
            "iter 2350: loss 1.1909, time 68.00ms, mfu 3.07%\n",
            "iter 2360: loss 1.1831, time 64.66ms, mfu 3.10%\n",
            "iter 2370: loss 1.2061, time 70.26ms, mfu 3.11%\n",
            "iter 2380: loss 1.1798, time 71.69ms, mfu 3.10%\n",
            "iter 2390: loss 1.1758, time 67.87ms, mfu 3.12%\n",
            "iter 2400: loss 1.1843, time 67.05ms, mfu 3.13%\n",
            "iter 2410: loss 1.1832, time 66.17ms, mfu 3.15%\n",
            "iter 2420: loss 1.1987, time 69.69ms, mfu 3.15%\n",
            "iter 2430: loss 1.1706, time 68.56ms, mfu 3.16%\n",
            "iter 2440: loss 1.1830, time 69.70ms, mfu 3.16%\n",
            "iter 2450: loss 1.1878, time 67.25ms, mfu 3.17%\n",
            "iter 2460: loss 1.1738, time 70.23ms, mfu 3.17%\n",
            "iter 2470: loss 1.1872, time 69.36ms, mfu 3.17%\n",
            "iter 2480: loss 1.1496, time 69.45ms, mfu 3.17%\n",
            "iter 2490: loss 1.1783, time 71.02ms, mfu 3.16%\n",
            "step 2500: train loss 1.0813, val loss 1.4625\n",
            "iter 2500: loss 1.1822, time 8379.82ms, mfu 2.85%\n",
            "iter 2510: loss 1.2076, time 69.39ms, mfu 2.88%\n",
            "iter 2520: loss 1.1298, time 68.67ms, mfu 2.91%\n",
            "iter 2530: loss 1.1551, time 65.67ms, mfu 2.96%\n",
            "iter 2540: loss 1.1804, time 68.89ms, mfu 2.98%\n",
            "iter 2550: loss 1.1593, time 70.08ms, mfu 3.00%\n",
            "iter 2560: loss 1.1503, time 72.02ms, mfu 3.00%\n",
            "iter 2570: loss 1.1933, time 67.59ms, mfu 3.03%\n",
            "iter 2580: loss 1.1777, time 68.77ms, mfu 3.05%\n",
            "iter 2590: loss 1.1577, time 66.85ms, mfu 3.07%\n",
            "iter 2600: loss 1.1528, time 68.41ms, mfu 3.09%\n",
            "iter 2610: loss 1.1624, time 67.90ms, mfu 3.10%\n",
            "iter 2620: loss 1.1455, time 70.50ms, mfu 3.10%\n",
            "iter 2630: loss 1.1621, time 70.57ms, mfu 3.10%\n",
            "iter 2640: loss 1.1423, time 70.73ms, mfu 3.11%\n",
            "iter 2650: loss 1.1509, time 71.11ms, mfu 3.10%\n",
            "iter 2660: loss 1.1644, time 69.47ms, mfu 3.11%\n",
            "iter 2670: loss 1.1671, time 66.07ms, mfu 3.13%\n",
            "iter 2680: loss 1.1405, time 66.75ms, mfu 3.15%\n",
            "iter 2690: loss 1.1490, time 70.92ms, mfu 3.15%\n",
            "iter 2700: loss 1.1487, time 68.42ms, mfu 3.15%\n",
            "iter 2710: loss 1.1478, time 67.83ms, mfu 3.16%\n",
            "iter 2720: loss 1.1972, time 67.02ms, mfu 3.17%\n",
            "iter 2730: loss 1.1564, time 68.23ms, mfu 3.18%\n",
            "iter 2740: loss 1.1559, time 68.33ms, mfu 3.18%\n",
            "step 2750: train loss 1.0488, val loss 1.4560\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.1560, time 8623.08ms, mfu 2.87%\n",
            "iter 2760: loss 1.1672, time 62.39ms, mfu 2.93%\n",
            "iter 2770: loss 1.1701, time 65.85ms, mfu 2.97%\n",
            "iter 2780: loss 1.1498, time 78.34ms, mfu 2.96%\n",
            "iter 2790: loss 1.1686, time 65.58ms, mfu 3.00%\n",
            "iter 2800: loss 1.1505, time 66.60ms, mfu 3.03%\n",
            "iter 2810: loss 1.1498, time 72.66ms, mfu 3.03%\n",
            "iter 2820: loss 1.1474, time 64.92ms, mfu 3.07%\n",
            "iter 2830: loss 1.1577, time 68.85ms, mfu 3.08%\n",
            "iter 2840: loss 1.1561, time 69.66ms, mfu 3.09%\n",
            "iter 2850: loss 1.1402, time 67.59ms, mfu 3.10%\n",
            "iter 2860: loss 1.1247, time 70.36ms, mfu 3.11%\n",
            "iter 2870: loss 1.1178, time 72.12ms, mfu 3.10%\n",
            "iter 2880: loss 1.1429, time 69.70ms, mfu 3.11%\n",
            "iter 2890: loss 1.1277, time 69.69ms, mfu 3.11%\n",
            "iter 2900: loss 1.1170, time 69.73ms, mfu 3.12%\n",
            "iter 2910: loss 1.1400, time 69.72ms, mfu 3.12%\n",
            "iter 2920: loss 1.1376, time 68.99ms, mfu 3.13%\n",
            "iter 2930: loss 1.1339, time 70.78ms, mfu 3.13%\n",
            "iter 2940: loss 1.1054, time 70.84ms, mfu 3.12%\n",
            "iter 2950: loss 1.1412, time 69.57ms, mfu 3.13%\n",
            "iter 2960: loss 1.1231, time 67.18ms, mfu 3.14%\n",
            "iter 2970: loss 1.1264, time 68.86ms, mfu 3.15%\n",
            "iter 2980: loss 1.1072, time 68.68ms, mfu 3.15%\n",
            "iter 2990: loss 1.1358, time 68.61ms, mfu 3.16%\n",
            "step 3000: train loss 1.0241, val loss 1.4570\n",
            "iter 3000: loss 1.1419, time 8379.60ms, mfu 2.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --max_iters=3000 --n_layer=7 --n_head=5 --n_embd=320 --dtype=float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692y2ktY_Urn",
        "outputId": "0db1240e-319f-4887-9075-14e4878abee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: n_layer = 7\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 320\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 8.63M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 8,704,320 parameters\n",
            "num non-decayed parameter tensors: 15, with 4,800 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0427 06:44:35.313000 6609 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.3009, val loss 4.2987\n",
            "iter 0: loss 4.2747, time 30779.91ms, mfu -100.00%\n",
            "iter 10: loss 3.2142, time 92.76ms, mfu 3.32%\n",
            "iter 20: loss 2.8275, time 92.66ms, mfu 3.32%\n",
            "iter 30: loss 2.6607, time 92.15ms, mfu 3.32%\n",
            "iter 40: loss 2.5838, time 92.17ms, mfu 3.32%\n",
            "iter 50: loss 2.5534, time 96.62ms, mfu 3.31%\n",
            "iter 60: loss 2.5291, time 98.64ms, mfu 3.29%\n",
            "iter 70: loss 2.5050, time 100.18ms, mfu 3.27%\n",
            "iter 80: loss 2.4845, time 101.20ms, mfu 3.25%\n",
            "iter 90: loss 2.4712, time 101.99ms, mfu 3.22%\n",
            "iter 100: loss 2.4752, time 101.21ms, mfu 3.21%\n",
            "iter 110: loss 2.4514, time 100.90ms, mfu 3.19%\n",
            "iter 120: loss 2.4610, time 100.72ms, mfu 3.18%\n",
            "iter 130: loss 2.4392, time 99.58ms, mfu 3.17%\n",
            "iter 140: loss 2.4326, time 100.98ms, mfu 3.16%\n",
            "iter 150: loss 2.4380, time 103.15ms, mfu 3.14%\n",
            "iter 160: loss 2.4060, time 101.86ms, mfu 3.13%\n",
            "iter 170: loss 2.3743, time 102.27ms, mfu 3.12%\n",
            "iter 180: loss 2.3639, time 101.36ms, mfu 3.11%\n",
            "iter 190: loss 2.3431, time 105.55ms, mfu 3.09%\n",
            "iter 200: loss 2.3430, time 102.71ms, mfu 3.08%\n",
            "iter 210: loss 2.2813, time 101.27ms, mfu 3.08%\n",
            "iter 220: loss 2.2847, time 101.59ms, mfu 3.07%\n",
            "iter 230: loss 2.2699, time 102.40ms, mfu 3.07%\n",
            "iter 240: loss 2.2241, time 101.17ms, mfu 3.06%\n",
            "step 250: train loss 2.1003, val loss 2.1618\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.1773, time 11972.89ms, mfu 2.76%\n",
            "iter 260: loss 2.1530, time 90.79ms, mfu 2.82%\n",
            "iter 270: loss 2.1201, time 93.78ms, mfu 2.87%\n",
            "iter 280: loss 2.0460, time 91.33ms, mfu 2.92%\n",
            "iter 290: loss 2.0384, time 90.32ms, mfu 2.97%\n",
            "iter 300: loss 2.0239, time 91.80ms, mfu 3.01%\n",
            "iter 310: loss 1.9888, time 92.77ms, mfu 3.04%\n",
            "iter 320: loss 1.9613, time 93.41ms, mfu 3.06%\n",
            "iter 330: loss 1.9262, time 92.77ms, mfu 3.09%\n",
            "iter 340: loss 1.9327, time 93.36ms, mfu 3.11%\n",
            "iter 350: loss 1.8857, time 94.28ms, mfu 3.13%\n",
            "iter 360: loss 1.9074, time 95.26ms, mfu 3.14%\n",
            "iter 370: loss 1.8573, time 94.57ms, mfu 3.15%\n",
            "iter 380: loss 1.8405, time 94.22ms, mfu 3.16%\n",
            "iter 390: loss 1.8397, time 95.02ms, mfu 3.17%\n",
            "iter 400: loss 1.8124, time 94.38ms, mfu 3.18%\n",
            "iter 410: loss 1.7971, time 91.93ms, mfu 3.20%\n",
            "iter 420: loss 1.7479, time 96.41ms, mfu 3.20%\n",
            "iter 430: loss 1.7337, time 92.40ms, mfu 3.21%\n",
            "iter 440: loss 1.7333, time 94.82ms, mfu 3.21%\n",
            "iter 450: loss 1.7423, time 93.53ms, mfu 3.22%\n",
            "iter 460: loss 1.6921, time 95.04ms, mfu 3.22%\n",
            "iter 470: loss 1.6971, time 96.62ms, mfu 3.22%\n",
            "iter 480: loss 1.6645, time 93.57ms, mfu 3.23%\n",
            "iter 490: loss 1.6771, time 91.62ms, mfu 3.24%\n",
            "step 500: train loss 1.5789, val loss 1.7449\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6541, time 11165.27ms, mfu 2.92%\n",
            "iter 510: loss 1.6431, time 89.79ms, mfu 2.97%\n",
            "iter 520: loss 1.6475, time 90.48ms, mfu 3.01%\n",
            "iter 530: loss 1.6293, time 88.89ms, mfu 3.06%\n",
            "iter 540: loss 1.6388, time 92.59ms, mfu 3.09%\n",
            "iter 550: loss 1.6038, time 90.53ms, mfu 3.12%\n",
            "iter 560: loss 1.5860, time 92.17ms, mfu 3.14%\n",
            "iter 570: loss 1.5931, time 92.73ms, mfu 3.16%\n",
            "iter 580: loss 1.5753, time 92.95ms, mfu 3.17%\n",
            "iter 590: loss 1.5786, time 94.30ms, mfu 3.18%\n",
            "iter 600: loss 1.5814, time 95.16ms, mfu 3.19%\n",
            "iter 610: loss 1.5643, time 93.34ms, mfu 3.20%\n",
            "iter 620: loss 1.5410, time 93.13ms, mfu 3.21%\n",
            "iter 630: loss 1.5358, time 97.50ms, mfu 3.20%\n",
            "iter 640: loss 1.5151, time 95.38ms, mfu 3.21%\n",
            "iter 650: loss 1.5303, time 94.01ms, mfu 3.21%\n",
            "iter 660: loss 1.5164, time 94.72ms, mfu 3.22%\n",
            "iter 670: loss 1.5082, time 92.33ms, mfu 3.23%\n",
            "iter 680: loss 1.5118, time 95.72ms, mfu 3.23%\n",
            "iter 690: loss 1.5310, time 98.22ms, mfu 3.22%\n",
            "iter 700: loss 1.5196, time 97.10ms, mfu 3.21%\n",
            "iter 710: loss 1.4925, time 97.32ms, mfu 3.21%\n",
            "iter 720: loss 1.5216, time 96.23ms, mfu 3.21%\n",
            "iter 730: loss 1.4569, time 92.76ms, mfu 3.22%\n",
            "iter 740: loss 1.4670, time 95.18ms, mfu 3.22%\n",
            "step 750: train loss 1.3876, val loss 1.6079\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4690, time 11554.63ms, mfu 2.90%\n",
            "iter 760: loss 1.4671, time 92.02ms, mfu 2.95%\n",
            "iter 770: loss 1.4528, time 94.39ms, mfu 2.98%\n",
            "iter 780: loss 1.4384, time 94.36ms, mfu 3.01%\n",
            "iter 790: loss 1.4594, time 96.41ms, mfu 3.03%\n",
            "iter 800: loss 1.4536, time 94.03ms, mfu 3.05%\n",
            "iter 810: loss 1.4334, time 96.05ms, mfu 3.07%\n",
            "iter 820: loss 1.4502, time 96.11ms, mfu 3.08%\n",
            "iter 830: loss 1.4119, time 95.10ms, mfu 3.10%\n",
            "iter 840: loss 1.4149, time 94.63ms, mfu 3.11%\n",
            "iter 850: loss 1.4402, time 94.43ms, mfu 3.13%\n",
            "iter 860: loss 1.4051, time 98.66ms, mfu 3.13%\n",
            "iter 870: loss 1.4233, time 96.48ms, mfu 3.13%\n",
            "iter 880: loss 1.4287, time 95.85ms, mfu 3.14%\n",
            "iter 890: loss 1.4009, time 98.14ms, mfu 3.14%\n",
            "iter 900: loss 1.4204, time 98.58ms, mfu 3.14%\n",
            "iter 910: loss 1.3742, time 99.66ms, mfu 3.13%\n",
            "iter 920: loss 1.3965, time 98.48ms, mfu 3.13%\n",
            "iter 930: loss 1.4221, time 100.13ms, mfu 3.13%\n",
            "iter 940: loss 1.3674, time 99.77ms, mfu 3.12%\n",
            "iter 950: loss 1.3783, time 101.16ms, mfu 3.12%\n",
            "iter 960: loss 1.3790, time 99.66ms, mfu 3.11%\n",
            "iter 970: loss 1.3512, time 98.41ms, mfu 3.11%\n",
            "iter 980: loss 1.3652, time 99.47ms, mfu 3.11%\n",
            "iter 990: loss 1.3422, time 96.04ms, mfu 3.12%\n",
            "step 1000: train loss 1.2839, val loss 1.5364\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3712, time 11450.28ms, mfu 2.81%\n",
            "iter 1010: loss 1.3794, time 93.79ms, mfu 2.86%\n",
            "iter 1020: loss 1.3662, time 89.83ms, mfu 2.92%\n",
            "iter 1030: loss 1.3272, time 91.46ms, mfu 2.96%\n",
            "iter 1040: loss 1.3788, time 90.58ms, mfu 3.01%\n",
            "iter 1050: loss 1.3522, time 92.69ms, mfu 3.04%\n",
            "iter 1060: loss 1.3316, time 93.41ms, mfu 3.06%\n",
            "iter 1070: loss 1.3364, time 91.72ms, mfu 3.09%\n",
            "iter 1080: loss 1.3379, time 97.24ms, mfu 3.10%\n",
            "iter 1090: loss 1.3588, time 96.44ms, mfu 3.11%\n",
            "iter 1100: loss 1.3601, time 94.08ms, mfu 3.13%\n",
            "iter 1110: loss 1.3340, time 96.07ms, mfu 3.13%\n",
            "iter 1120: loss 1.3508, time 92.59ms, mfu 3.15%\n",
            "iter 1130: loss 1.3480, time 95.13ms, mfu 3.16%\n",
            "iter 1140: loss 1.3269, time 95.62ms, mfu 3.17%\n",
            "iter 1150: loss 1.3082, time 92.97ms, mfu 3.18%\n",
            "iter 1160: loss 1.3184, time 95.24ms, mfu 3.19%\n",
            "iter 1170: loss 1.3632, time 96.82ms, mfu 3.19%\n",
            "iter 1180: loss 1.3085, time 98.71ms, mfu 3.18%\n",
            "iter 1190: loss 1.3222, time 98.96ms, mfu 3.17%\n",
            "iter 1200: loss 1.3058, time 97.19ms, mfu 3.17%\n",
            "iter 1210: loss 1.3098, time 91.24ms, mfu 3.19%\n",
            "iter 1220: loss 1.3236, time 92.99ms, mfu 3.20%\n",
            "iter 1230: loss 1.3128, time 94.16ms, mfu 3.21%\n",
            "iter 1240: loss 1.3027, time 96.30ms, mfu 3.21%\n",
            "step 1250: train loss 1.2208, val loss 1.5073\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2829, time 11391.58ms, mfu 2.89%\n",
            "iter 1260: loss 1.3032, time 89.80ms, mfu 2.95%\n",
            "iter 1270: loss 1.2522, time 91.75ms, mfu 2.99%\n",
            "iter 1280: loss 1.2960, time 92.74ms, mfu 3.02%\n",
            "iter 1290: loss 1.2680, time 91.69ms, mfu 3.05%\n",
            "iter 1300: loss 1.2701, time 92.04ms, mfu 3.08%\n",
            "iter 1310: loss 1.2934, time 90.08ms, mfu 3.12%\n",
            "iter 1320: loss 1.2873, time 92.29ms, mfu 3.14%\n",
            "iter 1330: loss 1.2948, time 89.69ms, mfu 3.17%\n",
            "iter 1340: loss 1.2741, time 93.61ms, mfu 3.18%\n",
            "iter 1350: loss 1.2821, time 92.88ms, mfu 3.19%\n",
            "iter 1360: loss 1.3009, time 97.85ms, mfu 3.19%\n",
            "iter 1370: loss 1.2540, time 96.74ms, mfu 3.19%\n",
            "iter 1380: loss 1.2645, time 92.46ms, mfu 3.20%\n",
            "iter 1390: loss 1.2786, time 94.47ms, mfu 3.21%\n",
            "iter 1400: loss 1.2695, time 98.04ms, mfu 3.20%\n",
            "iter 1410: loss 1.2768, time 93.29ms, mfu 3.21%\n",
            "iter 1420: loss 1.2418, time 97.21ms, mfu 3.21%\n",
            "iter 1430: loss 1.2744, time 97.37ms, mfu 3.20%\n",
            "iter 1440: loss 1.2655, time 98.19ms, mfu 3.20%\n",
            "iter 1450: loss 1.2571, time 96.26ms, mfu 3.20%\n",
            "iter 1460: loss 1.2545, time 95.56ms, mfu 3.20%\n",
            "iter 1470: loss 1.2359, time 95.69ms, mfu 3.20%\n",
            "iter 1480: loss 1.2761, time 93.20ms, mfu 3.21%\n",
            "iter 1490: loss 1.2476, time 92.72ms, mfu 3.22%\n",
            "step 1500: train loss 1.1698, val loss 1.4851\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.2472, time 11356.48ms, mfu 2.90%\n",
            "iter 1510: loss 1.2822, time 94.43ms, mfu 2.94%\n",
            "iter 1520: loss 1.2855, time 92.25ms, mfu 2.98%\n",
            "iter 1530: loss 1.2598, time 90.48ms, mfu 3.02%\n",
            "iter 1540: loss 1.2304, time 91.97ms, mfu 3.05%\n",
            "iter 1550: loss 1.2218, time 92.17ms, mfu 3.08%\n",
            "iter 1560: loss 1.2392, time 97.08ms, mfu 3.09%\n",
            "iter 1570: loss 1.2641, time 93.21ms, mfu 3.11%\n",
            "iter 1580: loss 1.2365, time 95.01ms, mfu 3.13%\n",
            "iter 1590: loss 1.2129, time 95.66ms, mfu 3.14%\n",
            "iter 1600: loss 1.2310, time 94.44ms, mfu 3.15%\n",
            "iter 1610: loss 1.2491, time 97.46ms, mfu 3.15%\n",
            "iter 1620: loss 1.2475, time 94.88ms, mfu 3.16%\n",
            "iter 1630: loss 1.2356, time 97.49ms, mfu 3.16%\n",
            "iter 1640: loss 1.2259, time 94.08ms, mfu 3.17%\n",
            "iter 1650: loss 1.2340, time 92.73ms, mfu 3.19%\n",
            "iter 1660: loss 1.2242, time 94.69ms, mfu 3.19%\n",
            "iter 1670: loss 1.2449, time 96.50ms, mfu 3.19%\n",
            "iter 1680: loss 1.2308, time 96.21ms, mfu 3.19%\n",
            "iter 1690: loss 1.1887, time 94.56ms, mfu 3.20%\n",
            "iter 1700: loss 1.2088, time 91.88ms, mfu 3.21%\n",
            "iter 1710: loss 1.2228, time 95.69ms, mfu 3.21%\n",
            "iter 1720: loss 1.1869, time 95.23ms, mfu 3.22%\n",
            "iter 1730: loss 1.2299, time 92.93ms, mfu 3.23%\n",
            "iter 1740: loss 1.2139, time 95.76ms, mfu 3.23%\n",
            "step 1750: train loss 1.1240, val loss 1.4693\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1952, time 11410.71ms, mfu 2.91%\n",
            "iter 1760: loss 1.1986, time 90.56ms, mfu 2.96%\n",
            "iter 1770: loss 1.2104, time 93.85ms, mfu 2.99%\n",
            "iter 1780: loss 1.2096, time 91.28ms, mfu 3.03%\n",
            "iter 1790: loss 1.1941, time 92.57ms, mfu 3.06%\n",
            "iter 1800: loss 1.2018, time 91.02ms, mfu 3.09%\n",
            "iter 1810: loss 1.2056, time 90.58ms, mfu 3.12%\n",
            "iter 1820: loss 1.2161, time 94.67ms, mfu 3.13%\n",
            "iter 1830: loss 1.1778, time 91.76ms, mfu 3.16%\n",
            "iter 1840: loss 1.1989, time 94.46ms, mfu 3.17%\n",
            "iter 1850: loss 1.1713, time 98.44ms, mfu 3.16%\n",
            "iter 1860: loss 1.1861, time 98.34ms, mfu 3.16%\n",
            "iter 1870: loss 1.1911, time 94.26ms, mfu 3.17%\n",
            "iter 1880: loss 1.1718, time 93.73ms, mfu 3.18%\n",
            "iter 1890: loss 1.1957, time 94.12ms, mfu 3.19%\n",
            "iter 1900: loss 1.1408, time 91.53ms, mfu 3.21%\n",
            "iter 1910: loss 1.2010, time 91.31ms, mfu 3.22%\n",
            "iter 1920: loss 1.2016, time 94.00ms, mfu 3.23%\n",
            "iter 1930: loss 1.1933, time 94.37ms, mfu 3.23%\n",
            "iter 1940: loss 1.1449, time 93.56ms, mfu 3.24%\n",
            "iter 1950: loss 1.1689, time 96.36ms, mfu 3.23%\n",
            "iter 1960: loss 1.1532, time 92.87ms, mfu 3.24%\n",
            "iter 1970: loss 1.1403, time 93.77ms, mfu 3.25%\n",
            "iter 1980: loss 1.1777, time 95.27ms, mfu 3.25%\n",
            "iter 1990: loss 1.1970, time 97.87ms, mfu 3.24%\n",
            "step 2000: train loss 1.0809, val loss 1.4675\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.1869, time 11455.19ms, mfu 2.91%\n",
            "iter 2010: loss 1.1870, time 91.09ms, mfu 2.96%\n",
            "iter 2020: loss 1.1732, time 90.77ms, mfu 3.00%\n",
            "iter 2030: loss 1.1291, time 93.88ms, mfu 3.03%\n",
            "iter 2040: loss 1.1561, time 93.67ms, mfu 3.06%\n",
            "iter 2050: loss 1.1757, time 93.78ms, mfu 3.08%\n",
            "iter 2060: loss 1.1427, time 92.05ms, mfu 3.11%\n",
            "iter 2070: loss 1.1463, time 92.30ms, mfu 3.13%\n",
            "iter 2080: loss 1.1752, time 91.31ms, mfu 3.15%\n",
            "iter 2090: loss 1.1595, time 94.64ms, mfu 3.16%\n",
            "iter 2100: loss 1.1415, time 97.14ms, mfu 3.16%\n",
            "iter 2110: loss 1.1475, time 92.73ms, mfu 3.18%\n",
            "iter 2120: loss 1.1126, time 93.75ms, mfu 3.19%\n",
            "iter 2130: loss 1.1408, time 98.59ms, mfu 3.18%\n",
            "iter 2140: loss 1.1300, time 98.02ms, mfu 3.18%\n",
            "iter 2150: loss 1.1594, time 94.39ms, mfu 3.19%\n",
            "iter 2160: loss 1.1625, time 95.36ms, mfu 3.19%\n",
            "iter 2170: loss 1.1414, time 92.73ms, mfu 3.21%\n",
            "iter 2180: loss 1.1420, time 92.91ms, mfu 3.22%\n",
            "iter 2190: loss 1.1361, time 95.52ms, mfu 3.22%\n",
            "iter 2200: loss 1.1461, time 93.30ms, mfu 3.23%\n",
            "iter 2210: loss 1.1687, time 94.24ms, mfu 3.23%\n",
            "iter 2220: loss 1.1468, time 96.61ms, mfu 3.23%\n",
            "iter 2230: loss 1.1405, time 99.45ms, mfu 3.21%\n",
            "iter 2240: loss 1.1453, time 95.03ms, mfu 3.22%\n",
            "step 2250: train loss 1.0450, val loss 1.4748\n",
            "iter 2250: loss 1.1226, time 11173.67ms, mfu 2.90%\n",
            "iter 2260: loss 1.1398, time 97.00ms, mfu 2.92%\n",
            "iter 2270: loss 1.1463, time 93.75ms, mfu 2.96%\n",
            "iter 2280: loss 1.1419, time 95.95ms, mfu 2.99%\n",
            "iter 2290: loss 1.1333, time 99.51ms, mfu 3.00%\n",
            "iter 2300: loss 1.1229, time 97.45ms, mfu 3.01%\n",
            "iter 2310: loss 1.0965, time 94.31ms, mfu 3.04%\n",
            "iter 2320: loss 1.1180, time 94.29ms, mfu 3.06%\n",
            "iter 2330: loss 1.1265, time 97.19ms, mfu 3.07%\n",
            "iter 2340: loss 1.1117, time 98.70ms, mfu 3.08%\n",
            "iter 2350: loss 1.1120, time 96.42ms, mfu 3.09%\n",
            "iter 2360: loss 1.0814, time 93.13ms, mfu 3.11%\n",
            "iter 2370: loss 1.1063, time 97.52ms, mfu 3.11%\n",
            "iter 2380: loss 1.0991, time 93.64ms, mfu 3.13%\n",
            "iter 2390: loss 1.1089, time 96.11ms, mfu 3.14%\n",
            "iter 2400: loss 1.1089, time 95.04ms, mfu 3.15%\n",
            "iter 2410: loss 1.1381, time 96.14ms, mfu 3.15%\n",
            "iter 2420: loss 1.1091, time 94.60ms, mfu 3.16%\n",
            "iter 2430: loss 1.1079, time 95.34ms, mfu 3.17%\n",
            "iter 2440: loss 1.1413, time 99.13ms, mfu 3.16%\n",
            "iter 2450: loss 1.1063, time 96.18ms, mfu 3.17%\n",
            "iter 2460: loss 1.1398, time 96.18ms, mfu 3.17%\n",
            "iter 2470: loss 1.0901, time 96.69ms, mfu 3.17%\n",
            "iter 2480: loss 1.1238, time 98.47ms, mfu 3.17%\n",
            "iter 2490: loss 1.1040, time 96.42ms, mfu 3.17%\n",
            "step 2500: train loss 1.0016, val loss 1.4789\n",
            "iter 2500: loss 1.1162, time 11237.46ms, mfu 2.86%\n",
            "iter 2510: loss 1.1191, time 99.53ms, mfu 2.88%\n",
            "iter 2520: loss 1.0803, time 95.41ms, mfu 2.92%\n",
            "iter 2530: loss 1.0858, time 98.32ms, mfu 2.94%\n",
            "iter 2540: loss 1.1316, time 96.77ms, mfu 2.96%\n",
            "iter 2550: loss 1.0805, time 95.99ms, mfu 2.99%\n",
            "iter 2560: loss 1.0701, time 95.44ms, mfu 3.01%\n",
            "iter 2570: loss 1.1332, time 97.38ms, mfu 3.03%\n",
            "iter 2580: loss 1.0930, time 97.05ms, mfu 3.04%\n",
            "iter 2590: loss 1.0813, time 95.89ms, mfu 3.06%\n",
            "iter 2600: loss 1.0714, time 97.12ms, mfu 3.07%\n",
            "iter 2610: loss 1.1028, time 94.55ms, mfu 3.09%\n",
            "iter 2620: loss 1.0837, time 96.02ms, mfu 3.10%\n",
            "iter 2630: loss 1.0912, time 93.86ms, mfu 3.12%\n",
            "iter 2640: loss 1.0434, time 98.44ms, mfu 3.12%\n",
            "iter 2650: loss 1.0780, time 99.50ms, mfu 3.12%\n",
            "iter 2660: loss 1.0881, time 95.80ms, mfu 3.13%\n",
            "iter 2670: loss 1.0622, time 98.03ms, mfu 3.13%\n",
            "iter 2680: loss 1.0734, time 97.63ms, mfu 3.13%\n",
            "iter 2690: loss 1.0861, time 95.23ms, mfu 3.14%\n",
            "iter 2700: loss 1.0822, time 94.98ms, mfu 3.15%\n",
            "iter 2710: loss 1.0955, time 97.44ms, mfu 3.15%\n",
            "iter 2720: loss 1.0816, time 98.10ms, mfu 3.15%\n",
            "iter 2730: loss 1.0745, time 96.27ms, mfu 3.16%\n",
            "iter 2740: loss 1.0839, time 96.10ms, mfu 3.16%\n",
            "step 2750: train loss 0.9571, val loss 1.4961\n",
            "iter 2750: loss 1.0653, time 11259.14ms, mfu 2.85%\n",
            "iter 2760: loss 1.0474, time 97.85ms, mfu 2.88%\n",
            "iter 2770: loss 1.0748, time 94.38ms, mfu 2.92%\n",
            "iter 2780: loss 1.0389, time 94.86ms, mfu 2.95%\n",
            "iter 2790: loss 1.0270, time 94.55ms, mfu 2.98%\n",
            "iter 2800: loss 1.0764, time 94.70ms, mfu 3.01%\n",
            "iter 2810: loss 1.0780, time 98.02ms, mfu 3.02%\n",
            "iter 2820: loss 1.0850, time 96.77ms, mfu 3.04%\n",
            "iter 2830: loss 1.0479, time 96.42ms, mfu 3.05%\n",
            "iter 2840: loss 1.0650, time 95.42ms, mfu 3.07%\n",
            "iter 2850: loss 1.0539, time 97.50ms, mfu 3.08%\n",
            "iter 2860: loss 1.0685, time 94.93ms, mfu 3.10%\n",
            "iter 2870: loss 1.0463, time 97.06ms, mfu 3.10%\n",
            "iter 2880: loss 1.0692, time 96.17ms, mfu 3.11%\n",
            "iter 2890: loss 1.0478, time 96.49ms, mfu 3.12%\n",
            "iter 2900: loss 1.0103, time 93.61ms, mfu 3.14%\n",
            "iter 2910: loss 1.0523, time 94.88ms, mfu 3.15%\n",
            "iter 2920: loss 1.0366, time 98.31ms, mfu 3.15%\n",
            "iter 2930: loss 1.0124, time 97.50ms, mfu 3.15%\n",
            "iter 2940: loss 1.0405, time 93.72ms, mfu 3.16%\n",
            "iter 2950: loss 1.0382, time 95.70ms, mfu 3.17%\n",
            "iter 2960: loss 1.0305, time 97.83ms, mfu 3.17%\n",
            "iter 2970: loss 1.0842, time 95.46ms, mfu 3.17%\n",
            "iter 2980: loss 1.0094, time 93.73ms, mfu 3.18%\n",
            "iter 2990: loss 1.0470, time 96.30ms, mfu 3.18%\n",
            "step 3000: train loss 0.9197, val loss 1.5007\n",
            "iter 3000: loss 1.0497, time 11204.64ms, mfu 2.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "figures_dir = 'figures'\n",
        "\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "    print(f\"Folder '{figures_dir}' created!\")\n",
        "else:\n",
        "    print(f\"Folder '{figures_dir}' already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoehLbfGDDlL",
        "outputId": "43d9a9ec-038a-4fba-c9bf-f7dbec9ff6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'figures' created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "layers = [2, 3, 5, 7]\n",
        "losses = [1.3373, 1.2441, 1.1419, 1.0497]\n",
        "\n",
        "plt.plot(layers, losses, marker='o')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Loss at Iteration 3000')\n",
        "plt.title('Loss vs Number of Layers (Heads fixed at 5)')\n",
        "plt.savefig('figures/loss_vs_layers.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "EHIm9ETaB55B",
        "outputId": "23debbdb-54a4-40e4-cb6b-3c7fcc4e7f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp -r data/shakespeare_char data/code_generation"
      ],
      "metadata": {
        "id": "QxIkdpJMFO6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load streaming dataset filtered for Python files\n",
        "ds = load_dataset(\"codeparrot/github-code\", split=\"train\", streaming=True, languages=[\"Python\"])\n",
        "\n",
        "# Collect enough code snippets to reach at least 100,000 tokens (characters)\n",
        "code_samples = []\n",
        "token_count = 0\n",
        "min_tokens = 100000\n",
        "\n",
        "for sample in ds:\n",
        "    code = sample[\"code\"]\n",
        "    code_samples.append(code)\n",
        "    token_count += len(code)\n",
        "    if token_count >= min_tokens:\n",
        "        break\n",
        "\n",
        "# Write to input.txt\n",
        "with open(\"data/code_generation/input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\\n\".join(code_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280,
          "referenced_widgets": [
            "5fad486466934ee5b474d2f63f380b50",
            "10585e296bf4416e8ed38a44f9b1dda4",
            "60cdffbe29134cb197bb8c419e1e8fea",
            "f9fe8d7c591046c68576ba3318f0621a",
            "87d49bb21b2e4889a49db8cdcf6a3325",
            "c9d898204d6a473fa953df8a868f85c5",
            "5e97ffce70344b45a7737ccb67c43f52",
            "7c266a94807349588aa7745bf4c4358f",
            "9794d1f79a4440a496c6351cec632e0c",
            "4c1c611575bc40c493a83136e7a9a896",
            "223bdceb505443829136c47ab9b4a563",
            "191d22a8569244d7a35720cf84bd43fa",
            "680cf774d2594723ae8d7398565fef4a",
            "454bb85e5c4a4cf7a5ee86323f4f0e75",
            "8e28c8eda7944d219257a892cfecda72",
            "16f051b0dd364cf3b35127a39c12134e",
            "c77e300774fc4853949429cd19818040",
            "8b56b66ee6254b72bcaf1a94450ec4ce",
            "9ba110ee29f449039fdb0e1ee6685dd1",
            "f470ce7540f740b8b7779f98a028faa3",
            "a4997c16b34a4182aaf4f01b1563dea0",
            "8416061564c64787875b0ecf76803e2c"
          ]
        },
        "id": "XFEs6YapFbNY",
        "outputId": "3c26967a-b9d1-4993-b834-264dc5318a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fad486466934ee5b474d2f63f380b50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "github-code.py:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "191d22a8569244d7a35720cf84bd43fa"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for codeparrot/github-code contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/codeparrot/github-code.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/code_generation/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YanyPvQPFnn6",
        "outputId": "5dc2b865-e7f9-4a14-8036-5efd5465c683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 106,762\n",
            "all the unique characters: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~–\n",
            "vocab size: 97\n",
            "train has 96,085 tokens\n",
            "val has 10,677 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_code_generation.py --max_iters=3000 --n_layer=7 --n_head=5 --n_embd=320 --dtype=float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJbQkOs4Hiox",
        "outputId": "fea72ac1-ba9b-4220-b0a3-1c1c895a81aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_code_generation.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-code-generation'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'code-generation'\n",
            "wandb_run_name = 'baby-gpt-code'\n",
            "\n",
            "dataset = 'code_generation'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: n_layer = 7\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 320\n",
            "Overriding: dtype = float16\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 97 (inside data/code_generation/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 8.64M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 8,714,560 parameters\n",
            "num non-decayed parameter tensors: 15, with 4,800 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0427 08:18:40.708000 30704 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "step 0: train loss 4.4849, val loss 4.2317\n",
            "iter 0: loss 4.5038, time 41873.16ms, mfu -100.00%\n",
            "iter 10: loss 3.5332, time 79.19ms, mfu 3.89%\n",
            "iter 20: loss 3.0959, time 82.41ms, mfu 3.88%\n",
            "iter 30: loss 2.9236, time 79.29ms, mfu 3.88%\n",
            "iter 40: loss 2.6920, time 83.31ms, mfu 3.86%\n",
            "iter 50: loss 2.6027, time 88.10ms, mfu 3.82%\n",
            "iter 60: loss 2.4687, time 79.12ms, mfu 3.83%\n",
            "iter 70: loss 2.4562, time 82.95ms, mfu 3.82%\n",
            "iter 80: loss 2.4261, time 79.37ms, mfu 3.83%\n",
            "iter 90: loss 2.2636, time 78.98ms, mfu 3.83%\n",
            "iter 100: loss 2.1731, time 83.08ms, mfu 3.82%\n",
            "iter 110: loss 2.2077, time 82.30ms, mfu 3.81%\n",
            "iter 120: loss 2.2093, time 79.35ms, mfu 3.82%\n",
            "iter 130: loss 2.0019, time 84.92ms, mfu 3.80%\n",
            "iter 140: loss 1.9829, time 81.63ms, mfu 3.80%\n",
            "iter 150: loss 2.0293, time 81.69ms, mfu 3.80%\n",
            "iter 160: loss 1.9452, time 87.12ms, mfu 3.77%\n",
            "iter 170: loss 1.9482, time 85.52ms, mfu 3.75%\n",
            "iter 180: loss 1.6946, time 79.61ms, mfu 3.77%\n",
            "iter 190: loss 1.7615, time 79.43ms, mfu 3.78%\n",
            "iter 200: loss 1.5516, time 83.76ms, mfu 3.77%\n",
            "iter 210: loss 1.5596, time 86.16ms, mfu 3.75%\n",
            "iter 220: loss 1.5175, time 82.65ms, mfu 3.75%\n",
            "iter 230: loss 1.5977, time 83.64ms, mfu 3.74%\n",
            "iter 240: loss 1.3408, time 84.62ms, mfu 3.73%\n",
            "step 250: train loss 1.1453, val loss 1.6169\n",
            "saving checkpoint to out-code-generation\n",
            "iter 250: loss 1.5019, time 10235.15ms, mfu 3.36%\n",
            "iter 260: loss 1.3390, time 82.14ms, mfu 3.40%\n",
            "iter 270: loss 1.2690, time 85.94ms, mfu 3.42%\n",
            "iter 280: loss 1.2164, time 85.62ms, mfu 3.44%\n",
            "iter 290: loss 1.1792, time 83.91ms, mfu 3.46%\n",
            "iter 300: loss 1.0214, time 88.28ms, mfu 3.46%\n",
            "iter 310: loss 1.0627, time 92.71ms, mfu 3.45%\n",
            "iter 320: loss 1.1197, time 91.17ms, mfu 3.44%\n",
            "iter 330: loss 1.0317, time 87.17ms, mfu 3.45%\n",
            "iter 340: loss 0.8835, time 87.97ms, mfu 3.46%\n",
            "iter 350: loss 0.9889, time 89.84ms, mfu 3.46%\n",
            "iter 360: loss 0.8674, time 87.67ms, mfu 3.46%\n",
            "iter 370: loss 0.8390, time 87.29ms, mfu 3.47%\n",
            "iter 380: loss 0.8568, time 87.92ms, mfu 3.47%\n",
            "iter 390: loss 0.8861, time 89.37ms, mfu 3.47%\n",
            "iter 400: loss 0.7451, time 85.80ms, mfu 3.48%\n",
            "iter 410: loss 0.6395, time 88.66ms, mfu 3.48%\n",
            "iter 420: loss 0.7375, time 88.17ms, mfu 3.48%\n",
            "iter 430: loss 0.7144, time 87.41ms, mfu 3.49%\n",
            "iter 440: loss 0.7344, time 91.82ms, mfu 3.47%\n",
            "iter 450: loss 0.6466, time 88.04ms, mfu 3.48%\n",
            "iter 460: loss 0.6327, time 90.63ms, mfu 3.47%\n",
            "iter 470: loss 0.5323, time 88.38ms, mfu 3.47%\n",
            "iter 480: loss 0.6184, time 92.38ms, mfu 3.46%\n",
            "iter 490: loss 0.5583, time 91.59ms, mfu 3.45%\n",
            "step 500: train loss 0.3711, val loss 1.8073\n",
            "iter 500: loss 0.5185, time 10851.16ms, mfu 3.11%\n",
            "iter 510: loss 0.4996, time 93.54ms, mfu 3.13%\n",
            "iter 520: loss 0.5224, time 94.35ms, mfu 3.14%\n",
            "iter 530: loss 0.5622, time 99.88ms, mfu 3.13%\n",
            "iter 540: loss 0.4800, time 93.99ms, mfu 3.15%\n",
            "iter 550: loss 0.4896, time 97.60ms, mfu 3.15%\n",
            "iter 560: loss 0.5246, time 97.25ms, mfu 3.15%\n",
            "iter 570: loss 0.4287, time 98.43ms, mfu 3.15%\n",
            "iter 580: loss 0.4393, time 97.84ms, mfu 3.15%\n",
            "iter 590: loss 0.4138, time 99.91ms, mfu 3.14%\n",
            "iter 600: loss 0.4250, time 96.59ms, mfu 3.15%\n",
            "iter 610: loss 0.3387, time 97.41ms, mfu 3.15%\n",
            "iter 620: loss 0.3840, time 96.18ms, mfu 3.16%\n",
            "iter 630: loss 0.3197, time 100.35ms, mfu 3.15%\n",
            "iter 640: loss 0.3715, time 99.29ms, mfu 3.14%\n",
            "iter 650: loss 0.3579, time 99.60ms, mfu 3.14%\n",
            "iter 660: loss 0.3419, time 100.21ms, mfu 3.13%\n",
            "iter 670: loss 0.3446, time 101.36ms, mfu 3.12%\n",
            "iter 680: loss 0.3338, time 101.30ms, mfu 3.12%\n",
            "iter 690: loss 0.3267, time 102.33ms, mfu 3.10%\n",
            "iter 700: loss 0.3257, time 100.93ms, mfu 3.10%\n",
            "iter 710: loss 0.3247, time 100.84ms, mfu 3.10%\n",
            "iter 720: loss 0.3285, time 102.23ms, mfu 3.09%\n",
            "iter 730: loss 0.3305, time 100.92ms, mfu 3.08%\n",
            "iter 740: loss 0.3038, time 99.85ms, mfu 3.08%\n",
            "step 750: train loss 0.1744, val loss 2.1018\n",
            "iter 750: loss 0.3191, time 11326.01ms, mfu 2.78%\n",
            "iter 760: loss 0.2760, time 98.44ms, mfu 2.81%\n",
            "iter 770: loss 0.2977, time 92.79ms, mfu 2.86%\n",
            "iter 780: loss 0.2581, time 94.27ms, mfu 2.91%\n",
            "iter 790: loss 0.2960, time 93.00ms, mfu 2.95%\n",
            "iter 800: loss 0.2764, time 94.31ms, mfu 2.98%\n",
            "iter 810: loss 0.2563, time 95.50ms, mfu 3.00%\n",
            "iter 820: loss 0.2491, time 94.42ms, mfu 3.03%\n",
            "iter 830: loss 0.2493, time 93.58ms, mfu 3.06%\n",
            "iter 840: loss 0.2430, time 94.18ms, mfu 3.08%\n",
            "iter 850: loss 0.2304, time 94.12ms, mfu 3.10%\n",
            "iter 860: loss 0.2139, time 95.65ms, mfu 3.11%\n",
            "iter 870: loss 0.2402, time 94.07ms, mfu 3.13%\n",
            "iter 880: loss 0.2421, time 93.98ms, mfu 3.14%\n",
            "iter 890: loss 0.2346, time 91.73ms, mfu 3.16%\n",
            "iter 900: loss 0.2154, time 92.53ms, mfu 3.18%\n",
            "iter 910: loss 0.2167, time 91.88ms, mfu 3.20%\n",
            "iter 920: loss 0.2234, time 96.39ms, mfu 3.20%\n",
            "iter 930: loss 0.2399, time 91.71ms, mfu 3.21%\n",
            "iter 940: loss 0.2179, time 93.24ms, mfu 3.22%\n",
            "iter 950: loss 0.2055, time 95.01ms, mfu 3.23%\n",
            "iter 960: loss 0.2172, time 92.71ms, mfu 3.24%\n",
            "iter 970: loss 0.2142, time 92.37ms, mfu 3.25%\n",
            "iter 980: loss 0.1975, time 94.20ms, mfu 3.25%\n",
            "iter 990: loss 0.2086, time 93.62ms, mfu 3.25%\n",
            "step 1000: train loss 0.1270, val loss 2.2095\n",
            "iter 1000: loss 0.1921, time 10882.69ms, mfu 2.93%\n",
            "iter 1010: loss 0.2085, time 93.18ms, mfu 2.97%\n",
            "iter 1020: loss 0.2256, time 90.21ms, mfu 3.01%\n",
            "iter 1030: loss 0.2015, time 92.79ms, mfu 3.04%\n",
            "iter 1040: loss 0.2075, time 95.38ms, mfu 3.06%\n",
            "iter 1050: loss 0.2038, time 92.86ms, mfu 3.09%\n",
            "iter 1060: loss 0.1833, time 96.30ms, mfu 3.10%\n",
            "iter 1070: loss 0.1909, time 97.07ms, mfu 3.11%\n",
            "iter 1080: loss 0.2124, time 95.80ms, mfu 3.12%\n",
            "iter 1090: loss 0.1828, time 96.87ms, mfu 3.12%\n",
            "iter 1100: loss 0.1889, time 91.84ms, mfu 3.15%\n",
            "iter 1110: loss 0.1895, time 93.74ms, mfu 3.16%\n",
            "iter 1120: loss 0.1721, time 93.61ms, mfu 3.18%\n",
            "iter 1130: loss 0.1754, time 93.31ms, mfu 3.19%\n",
            "iter 1140: loss 0.1729, time 93.61ms, mfu 3.20%\n",
            "iter 1150: loss 0.1743, time 91.74ms, mfu 3.21%\n",
            "iter 1160: loss 0.1841, time 93.04ms, mfu 3.22%\n",
            "iter 1170: loss 0.1684, time 94.62ms, mfu 3.23%\n",
            "iter 1180: loss 0.1856, time 92.21ms, mfu 3.24%\n",
            "iter 1190: loss 0.1809, time 96.54ms, mfu 3.23%\n",
            "iter 1200: loss 0.1691, time 96.06ms, mfu 3.23%\n",
            "iter 1210: loss 0.1773, time 92.77ms, mfu 3.24%\n",
            "iter 1220: loss 0.1638, time 95.96ms, mfu 3.24%\n",
            "iter 1230: loss 0.1866, time 95.06ms, mfu 3.24%\n",
            "iter 1240: loss 0.1701, time 94.05ms, mfu 3.24%\n",
            "step 1250: train loss 0.1069, val loss 2.1977\n",
            "iter 1250: loss 0.1725, time 11183.85ms, mfu 2.92%\n",
            "iter 1260: loss 0.1852, time 97.37ms, mfu 2.95%\n",
            "iter 1270: loss 0.1590, time 97.23ms, mfu 2.97%\n",
            "iter 1280: loss 0.1650, time 99.91ms, mfu 2.98%\n",
            "iter 1290: loss 0.1514, time 98.29ms, mfu 3.00%\n",
            "iter 1300: loss 0.1714, time 97.35ms, mfu 3.01%\n",
            "iter 1310: loss 0.1656, time 94.89ms, mfu 3.04%\n",
            "iter 1320: loss 0.1483, time 94.62ms, mfu 3.06%\n",
            "iter 1330: loss 0.1552, time 97.28ms, mfu 3.07%\n",
            "iter 1340: loss 0.1589, time 97.85ms, mfu 3.08%\n",
            "iter 1350: loss 0.1518, time 95.61ms, mfu 3.09%\n",
            "iter 1360: loss 0.1581, time 95.99ms, mfu 3.10%\n",
            "iter 1370: loss 0.1649, time 95.59ms, mfu 3.12%\n",
            "iter 1380: loss 0.1503, time 96.40ms, mfu 3.12%\n",
            "iter 1390: loss 0.1637, time 96.08ms, mfu 3.13%\n",
            "iter 1400: loss 0.1525, time 95.84ms, mfu 3.14%\n",
            "iter 1410: loss 0.1436, time 98.71ms, mfu 3.14%\n",
            "iter 1420: loss 0.1460, time 97.47ms, mfu 3.14%\n",
            "iter 1430: loss 0.1460, time 95.07ms, mfu 3.15%\n",
            "iter 1440: loss 0.1509, time 96.55ms, mfu 3.16%\n",
            "iter 1450: loss 0.1420, time 92.86ms, mfu 3.17%\n",
            "iter 1460: loss 0.1570, time 97.27ms, mfu 3.17%\n",
            "iter 1470: loss 0.1481, time 96.11ms, mfu 3.18%\n",
            "iter 1480: loss 0.1427, time 98.64ms, mfu 3.17%\n",
            "iter 1490: loss 0.1375, time 95.69ms, mfu 3.18%\n",
            "step 1500: train loss 0.0898, val loss 2.2655\n",
            "iter 1500: loss 0.1426, time 11100.61ms, mfu 2.86%\n",
            "iter 1510: loss 0.1409, time 99.94ms, mfu 2.88%\n",
            "iter 1520: loss 0.1431, time 97.43ms, mfu 2.91%\n",
            "iter 1530: loss 0.1505, time 99.28ms, mfu 2.93%\n",
            "iter 1540: loss 0.1507, time 94.59ms, mfu 2.96%\n",
            "iter 1550: loss 0.1402, time 93.98ms, mfu 3.00%\n",
            "iter 1560: loss 0.1486, time 92.43ms, mfu 3.03%\n",
            "iter 1570: loss 0.1353, time 93.15ms, mfu 3.06%\n",
            "iter 1580: loss 0.1421, time 94.08ms, mfu 3.08%\n",
            "iter 1590: loss 0.1391, time 95.92ms, mfu 3.09%\n",
            "iter 1600: loss 0.1401, time 97.35ms, mfu 3.10%\n",
            "iter 1610: loss 0.1404, time 96.68ms, mfu 3.11%\n",
            "iter 1620: loss 0.1361, time 94.28ms, mfu 3.13%\n",
            "iter 1630: loss 0.1282, time 95.33ms, mfu 3.14%\n",
            "iter 1640: loss 0.1338, time 96.06ms, mfu 3.14%\n",
            "iter 1650: loss 0.1356, time 94.81ms, mfu 3.15%\n",
            "iter 1660: loss 0.1364, time 91.60ms, mfu 3.18%\n",
            "iter 1670: loss 0.1302, time 92.62ms, mfu 3.19%\n",
            "iter 1680: loss 0.1292, time 92.96ms, mfu 3.20%\n",
            "iter 1690: loss 0.1329, time 96.49ms, mfu 3.20%\n",
            "iter 1700: loss 0.1254, time 95.39ms, mfu 3.21%\n",
            "iter 1710: loss 0.1300, time 94.06ms, mfu 3.21%\n",
            "iter 1720: loss 0.1285, time 91.71ms, mfu 3.23%\n",
            "iter 1730: loss 0.1236, time 93.90ms, mfu 3.23%\n",
            "iter 1740: loss 0.1350, time 90.52ms, mfu 3.25%\n",
            "step 1750: train loss 0.0824, val loss 2.2859\n",
            "iter 1750: loss 0.1301, time 11080.28ms, mfu 2.93%\n",
            "iter 1760: loss 0.1222, time 94.87ms, mfu 2.96%\n",
            "iter 1770: loss 0.1206, time 90.91ms, mfu 3.00%\n",
            "iter 1780: loss 0.1273, time 94.75ms, mfu 3.03%\n",
            "iter 1790: loss 0.1191, time 93.12ms, mfu 3.06%\n",
            "iter 1800: loss 0.1279, time 95.40ms, mfu 3.07%\n",
            "iter 1810: loss 0.1188, time 98.78ms, mfu 3.08%\n",
            "iter 1820: loss 0.1190, time 94.94ms, mfu 3.10%\n",
            "iter 1830: loss 0.1259, time 94.03ms, mfu 3.11%\n",
            "iter 1840: loss 0.1172, time 96.43ms, mfu 3.12%\n",
            "iter 1850: loss 0.1214, time 95.30ms, mfu 3.13%\n",
            "iter 1860: loss 0.1318, time 92.95ms, mfu 3.15%\n",
            "iter 1870: loss 0.1227, time 93.81ms, mfu 3.17%\n",
            "iter 1880: loss 0.1210, time 98.64ms, mfu 3.16%\n",
            "iter 1890: loss 0.1199, time 98.05ms, mfu 3.16%\n",
            "iter 1900: loss 0.1094, time 94.16ms, mfu 3.17%\n",
            "iter 1910: loss 0.1156, time 96.97ms, mfu 3.17%\n",
            "iter 1920: loss 0.1181, time 98.14ms, mfu 3.17%\n",
            "iter 1930: loss 0.1167, time 96.64ms, mfu 3.17%\n",
            "iter 1940: loss 0.1206, time 98.47ms, mfu 3.17%\n",
            "iter 1950: loss 0.1159, time 96.78ms, mfu 3.17%\n",
            "iter 1960: loss 0.1130, time 93.70ms, mfu 3.18%\n",
            "iter 1970: loss 0.1115, time 97.28ms, mfu 3.18%\n",
            "iter 1980: loss 0.1212, time 97.43ms, mfu 3.18%\n",
            "iter 1990: loss 0.1211, time 95.63ms, mfu 3.18%\n",
            "step 2000: train loss 0.0778, val loss 2.3343\n",
            "iter 2000: loss 0.1074, time 11140.93ms, mfu 2.87%\n",
            "iter 2010: loss 0.1187, time 96.54ms, mfu 2.90%\n",
            "iter 2020: loss 0.1107, time 95.98ms, mfu 2.93%\n",
            "iter 2030: loss 0.1147, time 95.55ms, mfu 2.96%\n",
            "iter 2040: loss 0.1066, time 95.46ms, mfu 2.99%\n",
            "iter 2050: loss 0.1107, time 95.73ms, mfu 3.01%\n",
            "iter 2060: loss 0.1103, time 98.42ms, mfu 3.02%\n",
            "iter 2070: loss 0.1070, time 92.75ms, mfu 3.05%\n",
            "iter 2080: loss 0.1098, time 97.56ms, mfu 3.06%\n",
            "iter 2090: loss 0.1071, time 95.89ms, mfu 3.08%\n",
            "iter 2100: loss 0.1109, time 96.27ms, mfu 3.09%\n",
            "iter 2110: loss 0.1129, time 95.32ms, mfu 3.11%\n",
            "iter 2120: loss 0.1050, time 96.94ms, mfu 3.11%\n",
            "iter 2130: loss 0.1135, time 93.78ms, mfu 3.13%\n",
            "iter 2140: loss 0.1067, time 93.42ms, mfu 3.15%\n",
            "iter 2150: loss 0.1014, time 96.74ms, mfu 3.15%\n",
            "iter 2160: loss 0.1118, time 99.11ms, mfu 3.15%\n",
            "iter 2170: loss 0.1048, time 94.85ms, mfu 3.16%\n",
            "iter 2180: loss 0.0989, time 95.22ms, mfu 3.17%\n",
            "iter 2190: loss 0.1052, time 97.43ms, mfu 3.17%\n",
            "iter 2200: loss 0.1066, time 93.57ms, mfu 3.18%\n",
            "iter 2210: loss 0.1113, time 94.62ms, mfu 3.19%\n",
            "iter 2220: loss 0.1106, time 95.69ms, mfu 3.19%\n",
            "iter 2230: loss 0.1062, time 97.78ms, mfu 3.19%\n",
            "iter 2240: loss 0.1002, time 93.81ms, mfu 3.20%\n",
            "step 2250: train loss 0.0692, val loss 2.3620\n",
            "iter 2250: loss 0.1065, time 11149.16ms, mfu 2.88%\n",
            "iter 2260: loss 0.0949, time 94.15ms, mfu 2.92%\n",
            "iter 2270: loss 0.1050, time 96.34ms, mfu 2.95%\n",
            "iter 2280: loss 0.1112, time 94.87ms, mfu 2.98%\n",
            "iter 2290: loss 0.0943, time 98.12ms, mfu 2.99%\n",
            "iter 2300: loss 0.0962, time 97.28ms, mfu 3.01%\n",
            "iter 2310: loss 0.0924, time 96.20ms, mfu 3.03%\n",
            "iter 2320: loss 0.0953, time 96.60ms, mfu 3.05%\n",
            "iter 2330: loss 0.0981, time 96.89ms, mfu 3.06%\n",
            "iter 2340: loss 0.0940, time 99.95ms, mfu 3.06%\n",
            "iter 2350: loss 0.1031, time 94.78ms, mfu 3.08%\n",
            "iter 2360: loss 0.1012, time 96.81ms, mfu 3.09%\n",
            "iter 2370: loss 0.0943, time 98.14ms, mfu 3.10%\n",
            "iter 2380: loss 0.0965, time 94.69ms, mfu 3.11%\n",
            "iter 2390: loss 0.0906, time 97.51ms, mfu 3.12%\n",
            "iter 2400: loss 0.1031, time 96.26ms, mfu 3.13%\n",
            "iter 2410: loss 0.1048, time 94.92ms, mfu 3.14%\n",
            "iter 2420: loss 0.0901, time 96.19ms, mfu 3.14%\n",
            "iter 2430: loss 0.0949, time 92.71ms, mfu 3.16%\n",
            "iter 2440: loss 0.0957, time 96.47ms, mfu 3.17%\n",
            "iter 2450: loss 0.0966, time 96.23ms, mfu 3.17%\n",
            "iter 2460: loss 0.1045, time 93.98ms, mfu 3.18%\n",
            "iter 2470: loss 0.0959, time 96.02ms, mfu 3.18%\n",
            "iter 2480: loss 0.0960, time 94.78ms, mfu 3.19%\n",
            "iter 2490: loss 0.0847, time 93.61ms, mfu 3.20%\n",
            "step 2500: train loss 0.0698, val loss 2.4793\n",
            "iter 2500: loss 0.0885, time 11138.67ms, mfu 2.88%\n",
            "iter 2510: loss 0.0980, time 97.83ms, mfu 2.91%\n",
            "iter 2520: loss 0.0869, time 97.45ms, mfu 2.94%\n",
            "iter 2530: loss 0.0961, time 93.93ms, mfu 2.97%\n",
            "iter 2540: loss 0.0854, time 94.91ms, mfu 3.00%\n",
            "iter 2550: loss 0.0989, time 97.98ms, mfu 3.01%\n",
            "iter 2560: loss 0.0896, time 95.96ms, mfu 3.03%\n",
            "iter 2570: loss 0.0902, time 94.55ms, mfu 3.06%\n",
            "iter 2580: loss 0.0902, time 95.70ms, mfu 3.07%\n",
            "iter 2590: loss 0.0900, time 97.24ms, mfu 3.08%\n",
            "iter 2600: loss 0.0903, time 94.16ms, mfu 3.10%\n",
            "iter 2610: loss 0.0924, time 92.94ms, mfu 3.12%\n",
            "iter 2620: loss 0.0887, time 96.18ms, mfu 3.13%\n",
            "iter 2630: loss 0.0952, time 91.60ms, mfu 3.15%\n",
            "iter 2640: loss 0.0899, time 91.61ms, mfu 3.18%\n",
            "iter 2650: loss 0.0913, time 92.36ms, mfu 3.19%\n",
            "iter 2660: loss 0.0862, time 96.38ms, mfu 3.19%\n",
            "iter 2670: loss 0.0870, time 94.63ms, mfu 3.20%\n",
            "iter 2680: loss 0.0908, time 97.21ms, mfu 3.20%\n",
            "iter 2690: loss 0.0902, time 96.59ms, mfu 3.20%\n",
            "iter 2700: loss 0.0897, time 93.02ms, mfu 3.21%\n",
            "iter 2710: loss 0.0930, time 94.83ms, mfu 3.21%\n",
            "iter 2720: loss 0.0911, time 94.66ms, mfu 3.22%\n",
            "iter 2730: loss 0.0849, time 93.93ms, mfu 3.22%\n",
            "iter 2740: loss 0.0772, time 96.02ms, mfu 3.22%\n",
            "step 2750: train loss 0.0648, val loss 2.4530\n",
            "iter 2750: loss 0.0884, time 11120.88ms, mfu 2.90%\n",
            "iter 2760: loss 0.0892, time 98.90ms, mfu 2.92%\n",
            "iter 2770: loss 0.0910, time 97.43ms, mfu 2.95%\n",
            "iter 2780: loss 0.0839, time 95.70ms, mfu 2.98%\n",
            "iter 2790: loss 0.0791, time 95.75ms, mfu 3.00%\n",
            "iter 2800: loss 0.0810, time 97.53ms, mfu 3.02%\n",
            "iter 2810: loss 0.0792, time 93.98ms, mfu 3.04%\n",
            "iter 2820: loss 0.0845, time 95.06ms, mfu 3.06%\n",
            "iter 2830: loss 0.0860, time 91.66ms, mfu 3.09%\n",
            "iter 2840: loss 0.0893, time 94.32ms, mfu 3.11%\n",
            "iter 2850: loss 0.0843, time 95.66ms, mfu 3.12%\n",
            "iter 2860: loss 0.0910, time 94.96ms, mfu 3.13%\n",
            "iter 2870: loss 0.0844, time 95.09ms, mfu 3.14%\n",
            "iter 2880: loss 0.0861, time 97.52ms, mfu 3.15%\n",
            "iter 2890: loss 0.0840, time 97.58ms, mfu 3.15%\n",
            "iter 2900: loss 0.0809, time 92.67ms, mfu 3.17%\n",
            "iter 2910: loss 0.0775, time 94.38ms, mfu 3.18%\n",
            "iter 2920: loss 0.0835, time 95.69ms, mfu 3.18%\n",
            "iter 2930: loss 0.0803, time 95.99ms, mfu 3.18%\n",
            "iter 2940: loss 0.0841, time 94.67ms, mfu 3.19%\n",
            "iter 2950: loss 0.0838, time 97.21ms, mfu 3.19%\n",
            "iter 2960: loss 0.0776, time 94.96ms, mfu 3.19%\n",
            "iter 2970: loss 0.0771, time 95.95ms, mfu 3.20%\n",
            "iter 2980: loss 0.0771, time 97.98ms, mfu 3.19%\n",
            "iter 2990: loss 0.0821, time 95.14ms, mfu 3.20%\n",
            "step 3000: train loss 0.0673, val loss 2.4599\n",
            "iter 3000: loss 0.0838, time 11104.92ms, mfu 2.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-code-generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgQGoLAnUlxf",
        "outputId": "01c93de6-b94a-4916-aedd-be9ba20a8bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-code-generation\n",
            "number of parameters: 8.64M\n",
            "Loading meta from data/code_generation/meta.pkl...\n",
            "\n",
            "\t\tjson_loct(key, val);\n",
            "\t\t\t\t\treturn NUL;\n",
            "\t}\n",
            "\n",
            "\n",
            "\t}\n",
            "\n",
            "\n",
            "bol json_din_dict(struct JsonValue *dict, const char **dst_p, char *va_p, char *key, str c_p, char *)\n",
            "{\n",
            "\tstruct MBuct JsonValue *jv;\n",
            "\tif (!val)\n",
            "\t\tif (!== valal;\n",
            "\treturn = 0;\n",
            "\n",
            "\tturn trrStralse(ctate = 0x->papate(ce, JstonVainte *valinte)\n",
            "\t\tif = (!== 0xES_A%)\n",
            "\t}\n",
            "\t\t\t->urete;\n",
            "}\n",
            "}\n",
            "\n",
            "bure at(ifingh (s[0 JSTRICON_CT, statate, *de, T TR_CON_KE, struchal)\n",
            "{\n",
            "\t\t\t\tc = = 0;\n",
            "\tretx->= mement_f, 0 kedey \"In(ctrex, JSON_LON_DICLICT);\n",
            "\t\t\treturn false;\n",
            "\tc = = JS_DIC\n",
            "---------------\n",
            "\n",
            "def = 0;\n",
            "\t\tif (!== 0 0 0 || epxE e == 0 v208 1;\n",
            "\t\t\titf (bufal >= 40;\n",
            "\tif (!== 01\n",
            "\tif esigned = 0x20 ign cht c = 0;\n",
            "\t\tc && (undended 0xDICON_errre_BON_DF8) {\n",
            "\t\til (*stx, '\\\\\\n\";\n",
            "\t\tc == sine '\\ne ';\n",
            "\tsse ';\n",
            "\t\tcase '(stx-'9');\n",
            "\tcase = ';\n",
            "\t\t\t\tcase ': '0'\\'n': case '\\ub'; ': easea': '[': '6':\n",
            "\t\t\t\t\t\t\t\t\t} easeak;\n",
            "\t\t\t\t\t\t\t\t\t}\n",
            "\t\tifareacak;\n",
            "\t\t\t\t\t} '\\ureaseace '1'< {\n",
            "\t\t\t\t\t\t\t\tMAPSTAPSTE(c+;\n",
            "\t\t\t\t\t\t}\n",
            "\t\t\t\t\t\n",
            "sease '<TA':\n",
            "\t\t\tcasease ':\n",
            "\t\t\t\tif'ute (!pasind_untertas['\", ');\n",
            "\t\t\t\t\t\t\t\t\t\tclsetxAPaserase(*d) T_pateterst\n",
            "---------------\n",
            "\n",
            "\t\tif (!srctx, key)\n",
            "\t\t\tre;\n",
            "\t\t\treturn == NULL;\n",
            "\t}\n",
            "\n",
            "\tc etxtx->pint = NUL;\n",
            "\t\treturn t;\n",
            "\tif (stf (!tateed ->pedseat, JSON_BON_DICT)\n",
            "\treturn false;\n",
            "\t\thif (!is_typee(bype3)\n",
            "\treacar(tx, \"Inval);\n",
            "\tcak;\n",
            "\tse = rcu.c {\n",
            "\t\tc tx->parent->parent_val, val *val;\n",
            "\treturn false;\n",
            "\tif (!c->= >ctx, JSONULON_L)\n",
            "\t\t\t\treturetx->ctur_ke;\n",
            "\t\treturrn N_false(ctx, JSTRON_L;\n",
            "\t\tif (!)\n",
            "\t\t\t\treturn false;\n",
            "\t}\n",
            "\t\t}\n",
            "\tif elset:\n",
            "\t\t\t\t\treturn->ur_n;\n",
            "}\n",
            "\t}\n",
            "\t\treturrn false;\n",
            "}\n",
            "\t}\n",
            "\n",
            "\n",
            "}\n",
            "}\n",
            "\n",
            "sooct.ev = ew_lopet_cuext(st(lct Jst *dex, p, chal *jvaso\n",
            "---------------\n",
            "\n",
            "\treturn *ctx->ct)\n",
            "\tre_ctx->pareng(ct);\n",
            "\t\treturn NULL;\n",
            "\t\tc = jv->u.ct;\n",
            "\t\t}\n",
            "\tif (in (inet->c->parerrr.ct, key_ctx, key, key, \"\"JSON_DON_DICT 002 falue cale);\n",
            "\t\tif (!conuntaiomer(ctions)\n",
            "\t\treturn S_ctxtx->paint;\n",
            "\t\tif (*ctx->parend;\n",
            "\t\t\tif (!parer_neror_aype(ct, \");\n",
            "\t\t\treturrn n faloalse;\n",
            "\t}\n",
            "\t}\n",
            "\n",
            "\treturrrn perrrr_.catenur 0];\n",
            "}\n",
            "\n",
            "}\n",
            "\n",
            "/* *\n",
            " ssercender fice */\n",
            "srstatatese */\n",
            " {\n",
            "statate s */\n",
            "\ts_lst_c;\n",
            "\t_condint *lexoat(st, ctrchat *d, char *d, c;\n",
            "\n",
            "\tst = = ensrcx;\n",
            "\t\t}\n",
            "\tn JSTRALLLONT_ON_CT,\n",
            "\t\t\t\tfase;\n",
            "\t\t\treto\n",
            "---------------\n",
            "\n",
            "\tretur_flset->tt->andem endit, bol ctend)\n",
            "\tval;\n",
            "\n",
            "\tif (!mbuf_ite(cteender, er_ewrt, key, key, JSON_LICT, lue))\n",
            "\treturn false;\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bool json_list_dict_pppt_dict_ew_fl(struct JsonValue *list, st char *dstonst conValue ***key, char const char ***dst_p)\n",
            "\tif (st_pt Jstyp, JsonVainValie *****vay, | ize_t_ty, indinext, Jst_t_vaindint, **vaizex, izent, iole_t,\n",
            "{\n",
            "\tthint\n",
            "\tvaint.valuene *cthasthas_p)\n",
            "\tvas_pt = = vasxt_s_poooloptat;\n",
            "\tstatruct JsonValue **jvaist;\n",
            "\tif d_geict_ding(lict, ct_p), stULin(st, JSO\n",
            "---------------\n",
            "\n",
            "\tit = iner.cthis.c = dendext() {\n",
            "\t});\n",
            "\t etect.__roddotthes();\n",
            "\t\tthit(f.is._meroutttt.__ates.pas(dde == r.___);\n",
            "\tretetue;\n",
            "\n",
            "\t}\n",
            "\n",
            "\n",
            "._inew == ruiontes();\n",
            "\n",
            "\n",
            ".dzopzone.dz-pzoss .dz-re {\n",
            "          osop;\n",
            "               opadenton: 0;\n",
            "        -webkit-t-transform: translanslateY(40px);\n",
            "          -mo-transfoz-trm: sforanslateY(40px);\n",
            "           -ms-transform: translateY(40px);\n",
            "         -ms-transfo-trm: translanslateY(40px);\n",
            "        -ms-transform: translateY(40px);\n",
            "         -ms-transforateY(40px);\n",
            "      -webk\n",
            "---------------\n",
            "\n",
            "   dect.ext(st);\n",
            "          });\n",
            "           }));\n",
            "        });\n",
            "         Object.defineProper(Clty \"pets(!= \"finalthig.___meninng \");\n",
            "   }\n",
            "           esetdCompont.es: 1;\n",
            "                scringht: 20.20.14px;\n",
            "   -manition: 1200;\n",
            "         -webkit-transform: translansleY(1.1.1.20px);\n",
            "        -ms-transfoz-trm: scanscaleY(0.1.1.0.0px);\n",
            "    itras .20% {\n",
            "     -webkit-t-transfoz-trm: scanscaleYiteY(-trm: transform: w: translanslateY(-40px);\n",
            "        transform: translateY(-te\\nsform: translateY(0px);\n",
            "       tra\n",
            "---------------\n",
            "\n",
            "     manslrg:     -manith {\n",
            "       -moz-transform: translateY(40px);\n",
            "            -ms-transform: translateY(-40px);\n",
            "            -ms-transform: translateY(-40px);\n",
            "              -ms-transform: translateY(-40px);\n",
            "          -moz-transform: translateY(0px);\n",
            "       }\n",
            "  0%  {\n",
            " 30%       20pacitity: {\n",
            " 0;\n",
            "            -webkit-t-transfoz-trmsforanslangslateY(0px);\n",
            "          -ms-transforateY(0px);\n",
            "        -o-transfoz-trm: translanslateY(40px);\n",
            "      -ms-transfoz-trm: translateY(0px);\n",
            "        -ms-transfo-teY\n",
            "---------------\n",
            "\n",
            "\t\treturn er_false(ctx, key)\n",
            "\t\t\treturn false;\n",
            "\n",
            "\tturn = jv->ct_dict;\n",
            "}\n",
            "\n",
            "static bol json_loolu_float(struct JsonValue *dict, const const char ***dstx, char const char ****g,\n",
            "{\n",
            "\tunst JsonContewaintext;\n",
            "\tchar *char *c;\n",
            "\tsrc = NUL;\n",
            "\tif (list->cu.ct re {\n",
            "\tif (ke >thaured;\n",
            "\t\tretur_false(ct >patx, Jsene_past, *kendstx, JstyponVaintypaline);\n",
            "\t}\n",
            "\tif (lst->pat->c->p ke_FLIN_e(in;\n",
            "\t\t\t*et = = ipt_chastypind(ct, JSOLICON_L;\n",
            "\t\t\tif et->urr->u.v_ke;\n",
            "\t}\n",
            "\t\t}\n",
            "\treturn e;\n",
            "\t}\n",
            "\treturnalsomby(c.ctharool->untr Liomexterra\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\tcholist_pt(struct JsonValue *jv)\n",
            "{\n",
            "\tstretuct JsonValue *dict;\n",
            "\tif (dict, key, key), falsey, &v, JSON_DICT)\n",
            "\treturn false;\n",
            "\tif (json_pape_t(jv)\n",
            "\treturn N_val, JSONULIST);\n",
            "}\n",
            "statate ic inext(struct JsonValue *jv)\n",
            "{\n",
            "\tstate = get_dict_gettext(jv), val);\n",
            "\n",
            "\tsoooizexterSt(g(v, fal->p, JSTON] \"IN_E &jvat);\n",
            "\t\treturn tue;\n",
            "}\n",
            "}\n",
            "\n",
            "son_lint_atypextypent_sext(struct JsonConContextext(*ctx, Jstx, conVainst **constx, chainst ***conuw_MBONG)\n",
            "{\n",
            "\tif f (une_t vaine_ext(val, *), dst_p)\n",
            "\t\treturn false;\n",
            "\tif (!jstyp J\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}